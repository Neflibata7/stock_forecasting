{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "012d121b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing AAPL ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.4284 | Test Loss: 0.5611\n",
      "Epoch 11/200 | Train Loss: 0.0756 | Test Loss: 0.3253\n",
      "Epoch 21/200 | Train Loss: 0.0440 | Test Loss: 0.1852\n",
      "Epoch 31/200 | Train Loss: 0.0298 | Test Loss: 0.3306\n",
      "Early stopping at epoch 35\n",
      "Epoch 1/200 | Train Loss: 0.4478 | Test Loss: 0.8256\n",
      "Epoch 11/200 | Train Loss: 0.2554 | Test Loss: 0.5544\n",
      "Epoch 21/200 | Train Loss: 0.1204 | Test Loss: 0.5925\n",
      "Early stopping at epoch 30\n",
      "Epoch 1/200 | Train Loss: 0.4794 | Test Loss: 0.5608\n",
      "Epoch 11/200 | Train Loss: 0.0711 | Test Loss: 0.1624\n",
      "Epoch 21/200 | Train Loss: 0.0461 | Test Loss: 0.1612\n",
      "Epoch 31/200 | Train Loss: 0.0425 | Test Loss: 0.1822\n",
      "Epoch 41/200 | Train Loss: 0.0390 | Test Loss: 0.1820\n",
      "Early stopping at epoch 47\n",
      "Epoch 1/200 | Train Loss: 0.4137 | Test Loss: 0.5616\n",
      "Epoch 11/200 | Train Loss: 0.2998 | Test Loss: 0.8074\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing MSFT ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.4150 | Test Loss: 0.5316\n",
      "Epoch 11/200 | Train Loss: 0.0859 | Test Loss: 0.2094\n",
      "Epoch 21/200 | Train Loss: 0.0609 | Test Loss: 0.2042\n",
      "Epoch 31/200 | Train Loss: 0.0400 | Test Loss: 0.2246\n",
      "Epoch 41/200 | Train Loss: 0.0308 | Test Loss: 0.1877\n",
      "Epoch 51/200 | Train Loss: 0.0220 | Test Loss: 0.1886\n",
      "Epoch 61/200 | Train Loss: 0.0177 | Test Loss: 0.2192\n",
      "Early stopping at epoch 62\n",
      "Epoch 1/200 | Train Loss: 0.4204 | Test Loss: 0.4955\n",
      "Epoch 11/200 | Train Loss: 0.2767 | Test Loss: 0.4563\n",
      "Epoch 21/200 | Train Loss: 0.1391 | Test Loss: 0.4480\n",
      "Epoch 31/200 | Train Loss: 0.0880 | Test Loss: 0.4881\n",
      "Early stopping at epoch 35\n",
      "Epoch 1/200 | Train Loss: 0.4140 | Test Loss: 0.5155\n",
      "Epoch 11/200 | Train Loss: 0.1090 | Test Loss: 0.2697\n",
      "Epoch 21/200 | Train Loss: 0.0572 | Test Loss: 0.2356\n",
      "Epoch 31/200 | Train Loss: 0.0568 | Test Loss: 0.2535\n",
      "Early stopping at epoch 39\n",
      "Epoch 1/200 | Train Loss: 0.4229 | Test Loss: 0.5008\n",
      "Epoch 11/200 | Train Loss: 0.3075 | Test Loss: 0.4808\n",
      "Epoch 21/200 | Train Loss: 0.2396 | Test Loss: 0.5892\n",
      "Early stopping at epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing AMZN ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.4211 | Test Loss: 0.5049\n",
      "Epoch 11/200 | Train Loss: 0.0753 | Test Loss: 0.1540\n",
      "Epoch 21/200 | Train Loss: 0.0368 | Test Loss: 0.1189\n",
      "Epoch 31/200 | Train Loss: 0.0290 | Test Loss: 0.1342\n",
      "Early stopping at epoch 37\n",
      "Epoch 1/200 | Train Loss: 0.3942 | Test Loss: 0.5287\n",
      "Epoch 11/200 | Train Loss: 0.2216 | Test Loss: 0.4297\n",
      "Epoch 21/200 | Train Loss: 0.1152 | Test Loss: 0.4561\n",
      "Early stopping at epoch 25\n",
      "Epoch 1/200 | Train Loss: 0.4060 | Test Loss: 0.5325\n",
      "Epoch 11/200 | Train Loss: 0.0766 | Test Loss: 0.1568\n",
      "Epoch 21/200 | Train Loss: 0.0467 | Test Loss: 0.2113\n",
      "Epoch 31/200 | Train Loss: 0.0395 | Test Loss: 0.1950\n",
      "Early stopping at epoch 38\n",
      "Epoch 1/200 | Train Loss: 0.4006 | Test Loss: 0.5354\n",
      "Epoch 11/200 | Train Loss: 0.2598 | Test Loss: 0.5327\n",
      "Epoch 21/200 | Train Loss: 0.1944 | Test Loss: 0.7789\n",
      "Early stopping at epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing GOOG ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.3611 | Test Loss: 0.5525\n",
      "Epoch 11/200 | Train Loss: 0.0806 | Test Loss: 0.1590\n",
      "Epoch 21/200 | Train Loss: 0.0478 | Test Loss: 0.1499\n",
      "Epoch 31/200 | Train Loss: 0.0351 | Test Loss: 0.1718\n",
      "Early stopping at epoch 32\n",
      "Epoch 1/200 | Train Loss: 0.3704 | Test Loss: 0.5445\n",
      "Epoch 11/200 | Train Loss: 0.2487 | Test Loss: 0.5413\n",
      "Epoch 21/200 | Train Loss: 0.1306 | Test Loss: 0.5117\n",
      "Early stopping at epoch 30\n",
      "Epoch 1/200 | Train Loss: 0.3875 | Test Loss: 0.5928\n",
      "Epoch 11/200 | Train Loss: 0.0711 | Test Loss: 0.1950\n",
      "Epoch 21/200 | Train Loss: 0.0518 | Test Loss: 0.2008\n",
      "Early stopping at epoch 24\n",
      "Epoch 1/200 | Train Loss: 0.3988 | Test Loss: 0.5582\n",
      "Epoch 11/200 | Train Loss: 0.2906 | Test Loss: 0.5246\n",
      "Epoch 21/200 | Train Loss: 0.2044 | Test Loss: 0.4853\n",
      "Epoch 31/200 | Train Loss: 0.1674 | Test Loss: 0.5661\n",
      "Epoch 41/200 | Train Loss: 0.1406 | Test Loss: 0.4734\n",
      "Early stopping at epoch 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing META ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.3763 | Test Loss: 0.6252\n",
      "Epoch 11/200 | Train Loss: 0.0776 | Test Loss: 0.2042\n",
      "Epoch 21/200 | Train Loss: 0.0449 | Test Loss: 0.1427\n",
      "Epoch 31/200 | Train Loss: 0.0329 | Test Loss: 0.2309\n",
      "Epoch 41/200 | Train Loss: 0.0235 | Test Loss: 0.2045\n",
      "Early stopping at epoch 47\n",
      "Epoch 1/200 | Train Loss: 0.3997 | Test Loss: 0.5561\n",
      "Epoch 11/200 | Train Loss: 0.2660 | Test Loss: 0.5975\n",
      "Epoch 21/200 | Train Loss: 0.1420 | Test Loss: 0.7466\n",
      "Early stopping at epoch 22\n",
      "Epoch 1/200 | Train Loss: 0.3786 | Test Loss: 0.6420\n",
      "Epoch 11/200 | Train Loss: 0.0715 | Test Loss: 0.1613\n",
      "Epoch 21/200 | Train Loss: 0.0467 | Test Loss: 0.1307\n",
      "Early stopping at epoch 29\n",
      "Epoch 1/200 | Train Loss: 0.3787 | Test Loss: 0.5407\n",
      "Epoch 11/200 | Train Loss: 0.2889 | Test Loss: 0.4733\n",
      "Epoch 21/200 | Train Loss: 0.2027 | Test Loss: 0.4299\n",
      "Epoch 31/200 | Train Loss: 0.1677 | Test Loss: 0.4791\n",
      "Early stopping at epoch 35\n",
      "\n",
      "=== Processing NVDA ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.3922 | Test Loss: 0.5912\n",
      "Epoch 11/200 | Train Loss: 0.0756 | Test Loss: 0.2324\n",
      "Epoch 21/200 | Train Loss: 0.0417 | Test Loss: 0.2256\n",
      "Epoch 31/200 | Train Loss: 0.0265 | Test Loss: 0.1463\n",
      "Epoch 41/200 | Train Loss: 0.0224 | Test Loss: 0.1490\n",
      "Early stopping at epoch 47\n",
      "Epoch 1/200 | Train Loss: 0.3787 | Test Loss: 0.6022\n",
      "Epoch 11/200 | Train Loss: 0.2444 | Test Loss: 0.6612\n",
      "Epoch 21/200 | Train Loss: 0.1038 | Test Loss: 0.5563\n",
      "Epoch 31/200 | Train Loss: 0.0762 | Test Loss: 0.6300\n",
      "Early stopping at epoch 33\n",
      "Epoch 1/200 | Train Loss: 0.3907 | Test Loss: 0.5955\n",
      "Epoch 11/200 | Train Loss: 0.0608 | Test Loss: 0.1686\n",
      "Epoch 21/200 | Train Loss: 0.0459 | Test Loss: 0.1909\n",
      "Epoch 31/200 | Train Loss: 0.0383 | Test Loss: 0.1967\n",
      "Early stopping at epoch 31\n",
      "Epoch 1/200 | Train Loss: 0.3690 | Test Loss: 0.5413\n",
      "Epoch 11/200 | Train Loss: 0.2805 | Test Loss: 0.5603\n",
      "Early stopping at epoch 15\n",
      "\n",
      "=== Processing TSLA ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.3839 | Test Loss: 0.4700\n",
      "Epoch 11/200 | Train Loss: 0.0709 | Test Loss: 0.1893\n",
      "Epoch 21/200 | Train Loss: 0.0413 | Test Loss: 0.1237\n",
      "Epoch 31/200 | Train Loss: 0.0296 | Test Loss: 0.1225\n",
      "Epoch 41/200 | Train Loss: 0.0237 | Test Loss: 0.1096\n",
      "Epoch 51/200 | Train Loss: 0.0163 | Test Loss: 0.1355\n",
      "Early stopping at epoch 54\n",
      "Epoch 1/200 | Train Loss: 0.3818 | Test Loss: 0.4879\n",
      "Epoch 11/200 | Train Loss: 0.2582 | Test Loss: 0.4908\n",
      "Epoch 21/200 | Train Loss: 0.1290 | Test Loss: 0.5897\n",
      "Epoch 31/200 | Train Loss: 0.0742 | Test Loss: 0.5611\n",
      "Early stopping at epoch 38\n",
      "Epoch 1/200 | Train Loss: 0.3966 | Test Loss: 0.5097\n",
      "Epoch 11/200 | Train Loss: 0.0696 | Test Loss: 0.1387\n",
      "Epoch 21/200 | Train Loss: 0.0434 | Test Loss: 0.1239\n",
      "Epoch 31/200 | Train Loss: 0.0382 | Test Loss: 0.1106\n",
      "Early stopping at epoch 36\n",
      "Epoch 1/200 | Train Loss: 0.3881 | Test Loss: 0.4842\n",
      "Epoch 11/200 | Train Loss: 0.2739 | Test Loss: 0.6647\n",
      "Early stopping at epoch 16\n",
      "\n",
      "=== Final Results ===\n",
      "\n",
      "AAPL Performance:\n",
      "Transformer:\n",
      "  RMSE: 0.6227 | MAE: 0.3919\n",
      "  Accuracy: 83.84% | Recall: 70.35%\n",
      "  Sharpe: -5.77 | Corr: 0.87\n",
      "  Precision: 0.98\n",
      "  F1: 0.82\n",
      "  AUC: 0.95\n",
      "  Confusion Matrix:\n",
      "[[313   5]\n",
      " [102 242]]\n",
      "Transformer_no_pe:\n",
      "  RMSE: 1.0236 | MAE: 0.7764\n",
      "  Accuracy: 66.01% | Recall: 43.60%\n",
      "  Sharpe: -9.37 | Corr: 0.49\n",
      "  Precision: 0.83\n",
      "  F1: 0.57\n",
      "  AUC: 0.73\n",
      "  Confusion Matrix:\n",
      "[[287  31]\n",
      " [194 150]]\n",
      "Informer:\n",
      "  RMSE: 0.4629 | MAE: 0.3302\n",
      "  Accuracy: 85.35% | Recall: 75.87%\n",
      "  Sharpe: -4.71 | Corr: 0.92\n",
      "  Precision: 0.95\n",
      "  F1: 0.84\n",
      "  AUC: 0.96\n",
      "  Confusion Matrix:\n",
      "[[304  14]\n",
      " [ 83 261]]\n",
      "Informer_no_pe:\n",
      "  RMSE: 1.0709 | MAE: 0.8171\n",
      "  Accuracy: 63.90% | Recall: 59.88%\n",
      "  Sharpe: 0.26 | Corr: 0.37\n",
      "  Precision: 0.67\n",
      "  F1: 0.63\n",
      "  AUC: 0.68\n",
      "  Confusion Matrix:\n",
      "[[217 101]\n",
      " [138 206]]\n",
      "\n",
      "MSFT Performance:\n",
      "Transformer:\n",
      "  RMSE: 0.4890 | MAE: 0.3266\n",
      "  Accuracy: 88.82% | Recall: 92.08%\n",
      "  Sharpe: 1.12 | Corr: 0.89\n",
      "  Precision: 0.87\n",
      "  F1: 0.89\n",
      "  AUC: 0.96\n",
      "  Confusion Matrix:\n",
      "[[274  47]\n",
      " [ 27 314]]\n",
      "Transformer_no_pe:\n",
      "  RMSE: 1.0023 | MAE: 0.7562\n",
      "  Accuracy: 63.44% | Recall: 65.69%\n",
      "  Sharpe: -0.66 | Corr: 0.39\n",
      "  Precision: 0.64\n",
      "  F1: 0.65\n",
      "  AUC: 0.69\n",
      "  Confusion Matrix:\n",
      "[[196 125]\n",
      " [117 224]]\n",
      "Informer:\n",
      "  RMSE: 0.5794 | MAE: 0.3625\n",
      "  Accuracy: 87.61% | Recall: 88.86%\n",
      "  Sharpe: -0.04 | Corr: 0.83\n",
      "  Precision: 0.87\n",
      "  F1: 0.88\n",
      "  AUC: 0.94\n",
      "  Confusion Matrix:\n",
      "[[277  44]\n",
      " [ 38 303]]\n",
      "Informer_no_pe:\n",
      "  RMSE: 1.1224 | MAE: 0.8398\n",
      "  Accuracy: 65.41% | Recall: 59.53%\n",
      "  Sharpe: -4.31 | Corr: 0.31\n",
      "  Precision: 0.69\n",
      "  F1: 0.64\n",
      "  AUC: 0.68\n",
      "  Confusion Matrix:\n",
      "[[230  91]\n",
      " [138 203]]\n",
      "\n",
      "AMZN Performance:\n",
      "Transformer:\n",
      "  RMSE: 0.4422 | MAE: 0.3156\n",
      "  Accuracy: 90.03% | Recall: 89.38%\n",
      "  Sharpe: -1.75 | Corr: 0.91\n",
      "  Precision: 0.91\n",
      "  F1: 0.90\n",
      "  AUC: 0.96\n",
      "  Confusion Matrix:\n",
      "[[293  30]\n",
      " [ 36 303]]\n",
      "Transformer_no_pe:\n",
      "  RMSE: 1.1182 | MAE: 0.8171\n",
      "  Accuracy: 67.82% | Recall: 80.83%\n",
      "  Sharpe: 4.85 | Corr: 0.46\n",
      "  Precision: 0.65\n",
      "  F1: 0.72\n",
      "  AUC: 0.73\n",
      "  Confusion Matrix:\n",
      "[[175 148]\n",
      " [ 65 274]]\n",
      "Informer:\n",
      "  RMSE: 0.5423 | MAE: 0.3716\n",
      "  Accuracy: 87.01% | Recall: 92.63%\n",
      "  Sharpe: 1.62 | Corr: 0.89\n",
      "  Precision: 0.84\n",
      "  F1: 0.88\n",
      "  AUC: 0.95\n",
      "  Confusion Matrix:\n",
      "[[262  61]\n",
      " [ 25 314]]\n",
      "Informer_no_pe:\n",
      "  RMSE: 1.1070 | MAE: 0.7791\n",
      "  Accuracy: 66.47% | Recall: 75.52%\n",
      "  Sharpe: 1.14 | Corr: 0.39\n",
      "  Precision: 0.65\n",
      "  F1: 0.70\n",
      "  AUC: 0.68\n",
      "  Confusion Matrix:\n",
      "[[184 139]\n",
      " [ 83 256]]\n",
      "\n",
      "GOOG Performance:\n",
      "Transformer:\n",
      "  RMSE: 0.5499 | MAE: 0.3858\n",
      "  Accuracy: 87.76% | Recall: 82.34%\n",
      "  Sharpe: -1.99 | Corr: 0.90\n",
      "  Precision: 0.93\n",
      "  F1: 0.87\n",
      "  AUC: 0.94\n",
      "  Confusion Matrix:\n",
      "[[306  22]\n",
      " [ 59 275]]\n",
      "Transformer_no_pe:\n",
      "  RMSE: 1.0121 | MAE: 0.7553\n",
      "  Accuracy: 66.16% | Recall: 66.47%\n",
      "  Sharpe: 0.45 | Corr: 0.54\n",
      "  Precision: 0.66\n",
      "  F1: 0.66\n",
      "  AUC: 0.73\n",
      "  Confusion Matrix:\n",
      "[[216 112]\n",
      " [112 222]]\n",
      "Informer:\n",
      "  RMSE: 0.6056 | MAE: 0.4418\n",
      "  Accuracy: 85.80% | Recall: 95.21%\n",
      "  Sharpe: 3.83 | Corr: 0.91\n",
      "  Precision: 0.80\n",
      "  F1: 0.87\n",
      "  AUC: 0.95\n",
      "  Confusion Matrix:\n",
      "[[250  78]\n",
      " [ 16 318]]\n",
      "Informer_no_pe:\n",
      "  RMSE: 1.1318 | MAE: 0.8180\n",
      "  Accuracy: 67.22% | Recall: 75.15%\n",
      "  Sharpe: 1.51 | Corr: 0.47\n",
      "  Precision: 0.65\n",
      "  F1: 0.70\n",
      "  AUC: 0.72\n",
      "  Confusion Matrix:\n",
      "[[194 134]\n",
      " [ 83 251]]\n",
      "\n",
      "META Performance:\n",
      "Transformer:\n",
      "  RMSE: 0.4439 | MAE: 0.3033\n",
      "  Accuracy: 90.05% | Recall: 89.04%\n",
      "  Sharpe: 1.07 | Corr: 0.92\n",
      "  Precision: 0.94\n",
      "  F1: 0.91\n",
      "  AUC: 0.96\n",
      "  Confusion Matrix:\n",
      "[[246  23]\n",
      " [ 41 333]]\n",
      "Transformer_no_pe:\n",
      "  RMSE: 1.0948 | MAE: 0.8065\n",
      "  Accuracy: 58.79% | Recall: 44.12%\n",
      "  Sharpe: -5.06 | Corr: 0.43\n",
      "  Precision: 0.75\n",
      "  F1: 0.55\n",
      "  AUC: 0.69\n",
      "  Confusion Matrix:\n",
      "[[213  56]\n",
      " [209 165]]\n",
      "Informer:\n",
      "  RMSE: 0.4726 | MAE: 0.3270\n",
      "  Accuracy: 89.42% | Recall: 91.98%\n",
      "  Sharpe: 2.15 | Corr: 0.93\n",
      "  Precision: 0.90\n",
      "  F1: 0.91\n",
      "  AUC: 0.95\n",
      "  Confusion Matrix:\n",
      "[[231  38]\n",
      " [ 30 344]]\n",
      "Informer_no_pe:\n",
      "  RMSE: 1.2236 | MAE: 0.8783\n",
      "  Accuracy: 62.83% | Recall: 62.83%\n",
      "  Sharpe: 2.12 | Corr: 0.31\n",
      "  Precision: 0.70\n",
      "  F1: 0.66\n",
      "  AUC: 0.67\n",
      "  Confusion Matrix:\n",
      "[[169 100]\n",
      " [139 235]]\n",
      "\n",
      "NVDA Performance:\n",
      "Transformer:\n",
      "  RMSE: 0.5628 | MAE: 0.4303\n",
      "  Accuracy: 86.56% | Recall: 83.29%\n",
      "  Sharpe: -1.14 | Corr: 0.90\n",
      "  Precision: 0.92\n",
      "  F1: 0.87\n",
      "  AUC: 0.95\n",
      "  Confusion Matrix:\n",
      "[[269  28]\n",
      " [ 61 304]]\n",
      "Transformer_no_pe:\n",
      "  RMSE: 1.2089 | MAE: 0.9100\n",
      "  Accuracy: 71.00% | Recall: 65.48%\n",
      "  Sharpe: -3.67 | Corr: 0.50\n",
      "  Precision: 0.78\n",
      "  F1: 0.71\n",
      "  AUC: 0.76\n",
      "  Confusion Matrix:\n",
      "[[231  66]\n",
      " [126 239]]\n",
      "Informer:\n",
      "  RMSE: 0.5906 | MAE: 0.4262\n",
      "  Accuracy: 84.89% | Recall: 84.93%\n",
      "  Sharpe: 1.01 | Corr: 0.87\n",
      "  Precision: 0.87\n",
      "  F1: 0.86\n",
      "  AUC: 0.92\n",
      "  Confusion Matrix:\n",
      "[[252  45]\n",
      " [ 55 310]]\n",
      "Informer_no_pe:\n",
      "  RMSE: 1.0730 | MAE: 0.8467\n",
      "  Accuracy: 65.41% | Recall: 70.96%\n",
      "  Sharpe: 2.46 | Corr: 0.42\n",
      "  Precision: 0.68\n",
      "  F1: 0.69\n",
      "  AUC: 0.69\n",
      "  Confusion Matrix:\n",
      "[[174 123]\n",
      " [106 259]]\n",
      "\n",
      "TSLA Performance:\n",
      "Transformer:\n",
      "  RMSE: 0.4114 | MAE: 0.3036\n",
      "  Accuracy: 89.12% | Recall: 83.00%\n",
      "  Sharpe: -2.46 | Corr: 0.93\n",
      "  Precision: 0.92\n",
      "  F1: 0.87\n",
      "  AUC: 0.96\n",
      "  Confusion Matrix:\n",
      "[[341  21]\n",
      " [ 51 249]]\n",
      "Transformer_no_pe:\n",
      "  RMSE: 1.1857 | MAE: 0.9214\n",
      "  Accuracy: 67.22% | Recall: 58.33%\n",
      "  Sharpe: -3.53 | Corr: 0.46\n",
      "  Precision: 0.66\n",
      "  F1: 0.62\n",
      "  AUC: 0.73\n",
      "  Confusion Matrix:\n",
      "[[270  92]\n",
      " [125 175]]\n",
      "Informer:\n",
      "  RMSE: 0.4488 | MAE: 0.3313\n",
      "  Accuracy: 86.10% | Recall: 74.00%\n",
      "  Sharpe: -4.65 | Corr: 0.94\n",
      "  Precision: 0.94\n",
      "  F1: 0.83\n",
      "  AUC: 0.96\n",
      "  Confusion Matrix:\n",
      "[[348  14]\n",
      " [ 78 222]]\n",
      "Informer_no_pe:\n",
      "  RMSE: 1.2502 | MAE: 0.9809\n",
      "  Accuracy: 67.98% | Recall: 84.00%\n",
      "  Sharpe: 5.18 | Corr: 0.52\n",
      "  Precision: 0.61\n",
      "  F1: 0.70\n",
      "  AUC: 0.77\n",
      "  Confusion Matrix:\n",
      "[[198 164]\n",
      " [ 48 252]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score, mean_squared_error, mean_absolute_error, f1_score, confusion_matrix, precision_score, roc_curve, auc\n",
    "from scipy.signal import savgol_filter\n",
    "from tqdm import tqdm\n",
    "from PyEMD import EEMD\n",
    "import time\n",
    "\n",
    "plt.rcParams['font.family'] = 'SimHei'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class Config:\n",
    "    symbols = ['AAPL', 'MSFT', 'AMZN', 'GOOG', 'META', 'NVDA', 'TSLA']\n",
    "    start_date = '2012-01-01'\n",
    "    end_date = pd.Timestamp.now().strftime('%Y-%m-%d')\n",
    "    seq_length = 20\n",
    "    train_ratio = 0.8\n",
    "    batch_size = 128\n",
    "    epochs = 200\n",
    "    lr = 5e-4\n",
    "    weight_decay = 1e-4\n",
    "    dropout = 0.2\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model_config = {\n",
    "        'Transformer': {'d_model': 128, 'nhead': 8, 'num_layers': 3},\n",
    "        'Informer': {'d_model': 128, 'nhead': 8, 'num_layers': 3}\n",
    "    }\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def get_data(self, symbol):\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                df = yf.download(symbol, start=Config.start_date, end=Config.end_date)\n",
    "                return df[['Open', 'High', 'Low', 'Close', 'Volume']].dropna()\n",
    "            except yf.YFRateLimitError:\n",
    "                print(f\"Rate limit reached for {symbol}. Retrying in 60 seconds...\")\n",
    "                time.sleep(60)\n",
    "        print(f\"Failed to download data for {symbol} after {max_retries} attempts.\")\n",
    "        return None\n",
    "\n",
    "    def compute_rsi(self, prices, period=14):\n",
    "        delta = prices.diff()\n",
    "        gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
    "        loss = -delta.where(delta < 0, 0).rolling(window=period).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    def compute_macd(self, prices, fast=12, slow=26, signal=9):\n",
    "        ema_fast = prices.ewm(span=fast, adjust=False).mean()\n",
    "        ema_slow = prices.ewm(span=slow, adjust=False).mean()\n",
    "        macd = ema_fast - ema_slow\n",
    "        signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
    "        return macd, signal_line\n",
    "\n",
    "    def add_features(self, df):\n",
    "        df['LogReturn'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "        df['MA5'] = df['Close'].rolling(5).mean()\n",
    "        df['MA20'] = df['Close'].rolling(20).mean()\n",
    "        df['RSI'] = self.compute_rsi(df['Close'])\n",
    "        macd, signal = self.compute_macd(df['Close'])\n",
    "        df['MACD'] = macd\n",
    "        df['MACD_Signal'] = signal\n",
    "        df['DayOfWeek'] = df.index.dayofweek\n",
    "        df['Month'] = df.index.month - 1\n",
    "        return df.dropna()\n",
    "\n",
    "    def decompose_with_eemd(self, series):\n",
    "        eemd = EEMD()\n",
    "        T = np.arange(len(series))\n",
    "        imfs = eemd.emd(series, T)\n",
    "        return imfs\n",
    "\n",
    "    def plot_eemd_decomposition(self, symbol, series, imfs):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(len(imfs) + 1, 1, 1)\n",
    "        plt.plot(series, label='series')\n",
    "        plt.title(f'{symbol} EEMD')\n",
    "        plt.legend()\n",
    "\n",
    "        for i, imf in enumerate(imfs):\n",
    "            plt.subplot(len(imfs) + 1, 1, i + 2)\n",
    "            plt.plot(imf, label=f'IMF_{i}')\n",
    "            plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{symbol}_eemd_decomposition.png')\n",
    "        plt.close()\n",
    "\n",
    "    def process_data(self, symbol):\n",
    "        df = self.get_data(symbol)\n",
    "        if df is None:\n",
    "            return None, None\n",
    "        df = self.add_features(df)\n",
    "        window_length = min(11, len(df['LogReturn']))\n",
    "        df['LogReturn'] = savgol_filter(df['LogReturn'], window_length, 2)\n",
    "\n",
    "        log_return_series = df['LogReturn'].values\n",
    "        imfs = self.decompose_with_eemd(log_return_series)\n",
    "        self.plot_eemd_decomposition(symbol, log_return_series, imfs)\n",
    "\n",
    "        for i, imf in enumerate(imfs):\n",
    "            df[f'IMF_{i}'] = imf\n",
    "\n",
    "        features = ['LogReturn', 'MA5', 'MA20', 'RSI', 'MACD', 'MACD_Signal', 'DayOfWeek', 'Month']\n",
    "        for i in range(len(imfs)):\n",
    "            features.append(f'IMF_{i}')\n",
    "        target = 'LogReturn'\n",
    "        df[features] = self.scaler.fit_transform(df[features])\n",
    "\n",
    "        X, y = [], []\n",
    "        for i in range(len(df) - Config.seq_length):\n",
    "            X.append(df[features].values[i:i + Config.seq_length])\n",
    "            y.append(df[target].values[i + Config.seq_length])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, d_model, nhead, num_layers, dropout, use_positional_encoding=True):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_size, d_model)\n",
    "        self.use_positional_encoding = use_positional_encoding\n",
    "        if use_positional_encoding:\n",
    "            self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, dim_feedforward=d_model * 4, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model, nhead, dim_feedforward=d_model * 4, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = nn.Linear(d_model, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        if self.use_positional_encoding:\n",
    "            x = self.pos_encoder(x)\n",
    "        memory = self.transformer_encoder(x)\n",
    "\n",
    "        tgt = x\n",
    "        if self.use_positional_encoding:\n",
    "            tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer_decoder(tgt, memory)\n",
    "\n",
    "        output = self.norm(output.mean(dim=1))\n",
    "        output = self.dropout(output)\n",
    "        return self.decoder(output)\n",
    "\n",
    "\n",
    "class ProbSparseAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, factor):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, nhead, dropout=Config.dropout, batch_first=True)\n",
    "        self.factor = factor\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        attn_output, _ = self.attention(q, k, v)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class InformerModel(nn.Module):\n",
    "    def __init__(self, input_size, d_model, nhead, num_layers, dropout, factor=5, use_positional_encoding=True):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_size, d_model)\n",
    "        self.use_positional_encoding = use_positional_encoding\n",
    "        if use_positional_encoding:\n",
    "            self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.attn_layers = nn.ModuleList([\n",
    "            ProbSparseAttention(d_model, nhead, factor) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model, nhead, dim_feedforward=d_model * 4, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = nn.Linear(d_model, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        if self.use_positional_encoding:\n",
    "            x = self.pos_encoder(x)\n",
    "        for attn in self.attn_layers:\n",
    "            x = attn(x, x, x)\n",
    "        memory = x\n",
    "\n",
    "        tgt = x\n",
    "        if self.use_positional_encoding:\n",
    "            tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer_decoder(tgt, memory)\n",
    "\n",
    "        output = self.norm(output.mean(dim=1))\n",
    "        output = self.dropout(output)\n",
    "        return self.decoder(output)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, model_name):\n",
    "        self.model = model.to(Config.device)\n",
    "        self.model_name = model_name\n",
    "        self.criterion = nn.HuberLoss(delta=1.0)\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=Config.lr, weight_decay=Config.weight_decay)\n",
    "        self.warmup_scheduler = optim.lr_scheduler.LinearLR(self.optimizer, start_factor=0.1, total_iters=10)\n",
    "        self.cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=Config.epochs - 10)\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop_counter = 0\n",
    "\n",
    "    def train_epoch(self, train_loader, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(Config.device), y.to(Config.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(X)\n",
    "            loss = self.criterion(outputs.squeeze(), y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if epoch < 10:\n",
    "            self.warmup_scheduler.step()\n",
    "        else:\n",
    "            self.cosine_scheduler.step()\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for X, y in test_loader:\n",
    "                X, y = X.to(Config.device), y.to(Config.device)\n",
    "                outputs = self.model(X)\n",
    "                loss = self.criterion(outputs.squeeze(), y)\n",
    "                total_loss += loss.item()\n",
    "                preds.extend(outputs.cpu().numpy().flatten())\n",
    "                truths.extend(y.cpu().numpy().flatten())\n",
    "        return total_loss / len(test_loader), np.array(preds), np.array(truths)\n",
    "\n",
    "    def train(self, train_loader, test_loader):\n",
    "        for epoch in range(Config.epochs):\n",
    "            train_loss = self.train_epoch(train_loader, epoch)\n",
    "            test_loss, preds, truths = self.evaluate(test_loader)\n",
    "            if test_loss < self.best_loss:\n",
    "                self.best_loss = test_loss\n",
    "                torch.save(self.model.state_dict(), f'best_{self.model_name}.pth')\n",
    "                self.early_stop_counter = 0\n",
    "            else:\n",
    "                self.early_stop_counter += 1\n",
    "                if self.early_stop_counter >= 15:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{Config.epochs} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "        self.model.load_state_dict(torch.load(f'best_{self.model_name}.pth'))\n",
    "        return preds, truths\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    @staticmethod\n",
    "    def calculate_metrics(preds, truths):\n",
    "        pred_dir = (preds > 0).astype(int)\n",
    "        true_dir = (truths > 0).astype(int)\n",
    "        fpr, tpr, thresholds = roc_curve(true_dir, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        metrics = {\n",
    "            'RMSE': np.sqrt(mean_squared_error(truths, preds)),\n",
    "            'MAE': mean_absolute_error(truths, preds),\n",
    "            'Accuracy': accuracy_score(true_dir, pred_dir),\n",
    "            'Recall': recall_score(true_dir, pred_dir),\n",
    "            'Sharpe': np.mean(preds) / np.std(preds) * np.sqrt(252) if np.std(preds) != 0 else 0,\n",
    "            'Correlation': np.corrcoef(truths, preds)[0, 1],\n",
    "            'F1': f1_score(true_dir, pred_dir),\n",
    "            'ConfusionMatrix': confusion_matrix(true_dir, pred_dir),\n",
    "            'Precision': precision_score(true_dir, pred_dir),\n",
    "            'AUC': roc_auc\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    processor = DataProcessor()\n",
    "    results = {}\n",
    "    for symbol in Config.symbols:\n",
    "        print(f\"\\n=== Processing {symbol} ===\")\n",
    "        X, y = processor.process_data(symbol)\n",
    "        if X is None or y is None:\n",
    "            continue\n",
    "        train_size = int(len(X) * Config.train_ratio)\n",
    "        X_train, X_test = X[:train_size], X[train_size:]\n",
    "        y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "        train_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "        test_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=Config.batch_size)\n",
    "\n",
    "        model_results = {}\n",
    "        input_size = X_train.shape[2]\n",
    "\n",
    "        # 原始Transformer模型\n",
    "        transformer = TransformerModel(input_size, **Config.model_config['Transformer'], dropout=Config.dropout, use_positional_encoding=True)\n",
    "        trans_trainer = Trainer(transformer, 'Transformer')\n",
    "        trans_preds, trans_truths = trans_trainer.train(train_loader, test_loader)\n",
    "        model_results['Transformer'] = Evaluator.calculate_metrics(trans_preds, trans_truths)\n",
    "\n",
    "        # 移除位置编码的Transformer模型\n",
    "        transformer_no_pe = TransformerModel(input_size, **Config.model_config['Transformer'], dropout=Config.dropout, use_positional_encoding=False)\n",
    "        trans_no_pe_trainer = Trainer(transformer_no_pe, 'Transformer_no_pe')\n",
    "        trans_no_pe_preds, trans_no_pe_truths = trans_no_pe_trainer.train(train_loader, test_loader)\n",
    "        model_results['Transformer_no_pe'] = Evaluator.calculate_metrics(trans_no_pe_preds, trans_no_pe_truths)\n",
    "\n",
    "        # 原始Informer模型\n",
    "        informer = InformerModel(input_size, **Config.model_config['Informer'], dropout=Config.dropout, use_positional_encoding=True)\n",
    "        informer_trainer = Trainer(informer, 'Informer')\n",
    "        informer_preds, informer_truths = informer_trainer.train(train_loader, test_loader)\n",
    "        model_results['Informer'] = Evaluator.calculate_metrics(informer_preds, informer_truths)\n",
    "\n",
    "        # 移除位置编码的Informer模型\n",
    "        informer_no_pe = InformerModel(input_size, **Config.model_config['Informer'], dropout=Config.dropout, use_positional_encoding=False)\n",
    "        informer_no_pe_trainer = Trainer(informer_no_pe, 'Informer_no_pe')\n",
    "        informer_no_pe_preds, informer_no_pe_truths = informer_no_pe_trainer.train(train_loader, test_loader)\n",
    "        model_results['Informer_no_pe'] = Evaluator.calculate_metrics(informer_no_pe_preds, informer_no_pe_truths)\n",
    "\n",
    "        results[symbol] = model_results\n",
    "\n",
    "        for model_name, preds in [('Transformer', trans_preds), ('Transformer_no_pe', trans_no_pe_preds), ('Informer', informer_preds), ('Informer_no_pe', informer_no_pe_preds)]:\n",
    "            # 绘制预测结果\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(trans_truths, label='True Returns', alpha=0.7)\n",
    "            plt.plot(preds, label=f'{model_name} Predicted Returns', linestyle='--')\n",
    "            plt.title(f'{symbol} Return Prediction - {model_name}')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'{symbol}_prediction_ending5_{model_name}.png')\n",
    "            plt.close()\n",
    "\n",
    "            # 绘制ROC曲线\n",
    "            pred_dir = (preds > 0).astype(int)\n",
    "            true_dir = (trans_truths > 0).astype(int)\n",
    "            fpr, tpr, thresholds = roc_curve(true_dir, preds)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title(f'{symbol} {model_name} ROC Curve')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.savefig(f'{symbol}_roc_curve_{model_name}.png')\n",
    "            plt.close()\n",
    "\n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    for symbol in results:\n",
    "        print(f\"\\n{symbol} Performance:\")\n",
    "        for model in results[symbol]:\n",
    "            metrics = results[symbol][model]\n",
    "            print(f\"{model}:\")\n",
    "            print(f\"  RMSE: {metrics['RMSE']:.4f} | MAE: {metrics['MAE']:.4f}\")\n",
    "            print(f\"  Accuracy: {metrics['Accuracy']:.2%} | Recall: {metrics['Recall']:.2%}\")\n",
    "            print(f\"  Sharpe: {metrics['Sharpe']:.2f} | Corr: {metrics['Correlation']:.2f}\")\n",
    "            print(f\"  Precision: {metrics['Precision']:.2f}\")\n",
    "            print(f\"  F1: {metrics['F1']:.2f}\")\n",
    "            print(f\"  AUC: {metrics['AUC']:.2f}\")\n",
    "            print(f\"  Confusion Matrix:\\n{metrics['ConfusionMatrix']}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e4671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
