{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26906aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing AAPL ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.4237 | Test Loss: 0.5543\n",
      "Epoch 11/200 | Train Loss: 0.0680 | Test Loss: 0.2099\n",
      "Epoch 21/200 | Train Loss: 0.0404 | Test Loss: 0.1646\n",
      "Epoch 31/200 | Train Loss: 0.0336 | Test Loss: 0.3269\n",
      "Early stopping at epoch 35\n",
      "Epoch 1/200 | Train Loss: 0.4508 | Test Loss: 0.7586\n",
      "Epoch 11/200 | Train Loss: 0.2511 | Test Loss: 0.5114\n",
      "Epoch 21/200 | Train Loss: 0.1231 | Test Loss: 0.5274\n",
      "Early stopping at epoch 30\n",
      "Epoch 1/200 | Train Loss: 0.4785 | Test Loss: 0.5616\n",
      "Epoch 11/200 | Train Loss: 0.0662 | Test Loss: 0.1591\n",
      "Epoch 21/200 | Train Loss: 0.0496 | Test Loss: 0.1564\n",
      "Epoch 31/200 | Train Loss: 0.0444 | Test Loss: 0.1728\n",
      "Epoch 41/200 | Train Loss: 0.0423 | Test Loss: 0.2241\n",
      "Early stopping at epoch 47\n",
      "Epoch 1/200 | Train Loss: 0.4084 | Test Loss: 0.5672\n",
      "Epoch 11/200 | Train Loss: 0.2911 | Test Loss: 0.6030\n",
      "Epoch 21/200 | Train Loss: 0.2064 | Test Loss: 0.6168\n",
      "Early stopping at epoch 28\n",
      "Epoch 1/200 | Train Loss: 0.4270 | Test Loss: 0.5575\n",
      "Epoch 11/200 | Train Loss: 0.0777 | Test Loss: 0.2024\n",
      "Epoch 21/200 | Train Loss: 0.0506 | Test Loss: 0.1877\n",
      "Epoch 31/200 | Train Loss: 0.0421 | Test Loss: 0.1394\n",
      "Epoch 41/200 | Train Loss: 0.0380 | Test Loss: 0.1692\n",
      "Early stopping at epoch 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.4180 | Test Loss: 0.5529\n",
      "Epoch 11/200 | Train Loss: 0.1522 | Test Loss: 0.2263\n",
      "Epoch 21/200 | Train Loss: 0.1114 | Test Loss: 0.1984\n",
      "Epoch 31/200 | Train Loss: 0.0917 | Test Loss: 0.2552\n",
      "Early stopping at epoch 37\n",
      "Epoch 1/200 | Train Loss: 0.4446 | Test Loss: 0.5566\n",
      "Epoch 11/200 | Train Loss: 0.1592 | Test Loss: 0.2580\n",
      "Epoch 21/200 | Train Loss: 0.1178 | Test Loss: 0.2261\n",
      "Early stopping at epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.4352 | Test Loss: 0.5558\n",
      "Epoch 11/200 | Train Loss: 0.1032 | Test Loss: 0.3379\n",
      "Epoch 21/200 | Train Loss: 0.0502 | Test Loss: 0.2384\n",
      "Epoch 31/200 | Train Loss: 0.0337 | Test Loss: 0.3521\n",
      "Early stopping at epoch 31\n",
      "Epoch 1/200 | Train Loss: 0.4332 | Test Loss: 0.5690\n",
      "Epoch 11/200 | Train Loss: 0.0829 | Test Loss: 0.2106\n",
      "Epoch 21/200 | Train Loss: 0.0571 | Test Loss: 0.2001\n",
      "Epoch 31/200 | Train Loss: 0.0437 | Test Loss: 0.2129\n",
      "Early stopping at epoch 37\n",
      "\n",
      "=== Processing MSFT ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.4172 | Test Loss: 0.5131\n",
      "Epoch 11/200 | Train Loss: 0.0898 | Test Loss: 0.2084\n",
      "Epoch 21/200 | Train Loss: 0.0500 | Test Loss: 0.2330\n",
      "Early stopping at epoch 25\n",
      "Epoch 1/200 | Train Loss: 0.4599 | Test Loss: 0.5509\n",
      "Epoch 11/200 | Train Loss: 0.2581 | Test Loss: 0.4821\n",
      "Epoch 21/200 | Train Loss: 0.1350 | Test Loss: 0.5408\n",
      "Epoch 31/200 | Train Loss: 0.0868 | Test Loss: 0.4560\n",
      "Early stopping at epoch 36\n",
      "Epoch 1/200 | Train Loss: 0.4481 | Test Loss: 0.5137\n",
      "Epoch 11/200 | Train Loss: 0.0753 | Test Loss: 0.2728\n",
      "Epoch 21/200 | Train Loss: 0.0679 | Test Loss: 0.2572\n",
      "Epoch 31/200 | Train Loss: 0.0512 | Test Loss: 0.2628\n",
      "Epoch 41/200 | Train Loss: 0.0442 | Test Loss: 0.2612\n",
      "Early stopping at epoch 48\n",
      "Epoch 1/200 | Train Loss: 0.4156 | Test Loss: 0.4851\n",
      "Epoch 11/200 | Train Loss: 0.3170 | Test Loss: 0.4439\n",
      "Epoch 21/200 | Train Loss: 0.2231 | Test Loss: 0.4660\n",
      "Early stopping at epoch 29\n",
      "Epoch 1/200 | Train Loss: 0.4178 | Test Loss: 0.5107\n",
      "Epoch 11/200 | Train Loss: 0.0784 | Test Loss: 0.2495\n",
      "Epoch 21/200 | Train Loss: 0.0541 | Test Loss: 0.2552\n",
      "Epoch 31/200 | Train Loss: 0.0461 | Test Loss: 0.2697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.4159 | Test Loss: 0.5250\n",
      "Epoch 11/200 | Train Loss: 0.1519 | Test Loss: 0.2448\n",
      "Epoch 21/200 | Train Loss: 0.1328 | Test Loss: 0.1998\n",
      "Epoch 31/200 | Train Loss: 0.0983 | Test Loss: 0.1984\n",
      "Early stopping at epoch 36\n",
      "Epoch 1/200 | Train Loss: 0.4537 | Test Loss: 0.5267\n",
      "Epoch 11/200 | Train Loss: 0.1398 | Test Loss: 0.2228\n",
      "Epoch 21/200 | Train Loss: 0.1231 | Test Loss: 0.2049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.4263 | Test Loss: 0.4870\n",
      "Epoch 11/200 | Train Loss: 0.0761 | Test Loss: 0.2117\n",
      "Epoch 21/200 | Train Loss: 0.0541 | Test Loss: 0.2283\n",
      "Early stopping at epoch 25\n",
      "Epoch 1/200 | Train Loss: 0.4118 | Test Loss: 0.5162\n",
      "Epoch 11/200 | Train Loss: 0.0933 | Test Loss: 0.2555\n",
      "Epoch 21/200 | Train Loss: 0.0611 | Test Loss: 0.2663\n",
      "Epoch 31/200 | Train Loss: 0.0519 | Test Loss: 0.2758\n",
      "Epoch 41/200 | Train Loss: 0.0505 | Test Loss: 0.2695\n",
      "Early stopping at epoch 43\n",
      "\n",
      "=== Processing AMZN ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.4473 | Test Loss: 0.5119\n",
      "Epoch 11/200 | Train Loss: 0.0722 | Test Loss: 0.1657\n",
      "Epoch 21/200 | Train Loss: 0.0388 | Test Loss: 0.1305\n",
      "Epoch 31/200 | Train Loss: 0.0290 | Test Loss: 0.1232\n",
      "Epoch 41/200 | Train Loss: 0.0186 | Test Loss: 0.1251\n",
      "Epoch 51/200 | Train Loss: 0.0155 | Test Loss: 0.1360\n",
      "Early stopping at epoch 59\n",
      "Epoch 1/200 | Train Loss: 0.4212 | Test Loss: 0.5260\n",
      "Epoch 11/200 | Train Loss: 0.2321 | Test Loss: 0.5415\n",
      "Epoch 21/200 | Train Loss: 0.1131 | Test Loss: 0.4659\n",
      "Epoch 31/200 | Train Loss: 0.0804 | Test Loss: 0.5624\n",
      "Early stopping at epoch 39\n",
      "Epoch 1/200 | Train Loss: 0.4097 | Test Loss: 0.5291\n",
      "Epoch 11/200 | Train Loss: 0.1180 | Test Loss: 0.2157\n",
      "Epoch 21/200 | Train Loss: 0.0572 | Test Loss: 0.1469\n",
      "Epoch 31/200 | Train Loss: 0.0395 | Test Loss: 0.1541\n",
      "Epoch 41/200 | Train Loss: 0.0334 | Test Loss: 0.1527\n",
      "Early stopping at epoch 43\n",
      "Epoch 1/200 | Train Loss: 0.3870 | Test Loss: 0.5093\n",
      "Epoch 11/200 | Train Loss: 0.2560 | Test Loss: 0.4961\n",
      "Epoch 21/200 | Train Loss: 0.1869 | Test Loss: 0.4478\n",
      "Early stopping at epoch 29\n",
      "Epoch 1/200 | Train Loss: 0.3976 | Test Loss: 0.5288\n",
      "Epoch 11/200 | Train Loss: 0.0827 | Test Loss: 0.1485\n",
      "Epoch 21/200 | Train Loss: 0.0486 | Test Loss: 0.1427\n",
      "Epoch 31/200 | Train Loss: 0.0385 | Test Loss: 0.1790\n",
      "Early stopping at epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.4049 | Test Loss: 0.5183\n",
      "Epoch 11/200 | Train Loss: 0.1248 | Test Loss: 0.1851\n",
      "Epoch 21/200 | Train Loss: 0.1013 | Test Loss: 0.2551\n",
      "Early stopping at epoch 25\n",
      "Epoch 1/200 | Train Loss: 0.4016 | Test Loss: 0.5272\n",
      "Epoch 11/200 | Train Loss: 0.1242 | Test Loss: 0.1883\n",
      "Epoch 21/200 | Train Loss: 0.1139 | Test Loss: 0.1904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.4017 | Test Loss: 0.5073\n",
      "Epoch 11/200 | Train Loss: 0.1008 | Test Loss: 0.1530\n",
      "Epoch 21/200 | Train Loss: 0.0655 | Test Loss: 0.1929\n",
      "Epoch 31/200 | Train Loss: 0.0286 | Test Loss: 0.1361\n",
      "Epoch 41/200 | Train Loss: 0.0238 | Test Loss: 0.1374\n",
      "Epoch 51/200 | Train Loss: 0.0186 | Test Loss: 0.1373\n",
      "Early stopping at epoch 52\n",
      "Epoch 1/200 | Train Loss: 0.4173 | Test Loss: 0.5260\n",
      "Epoch 11/200 | Train Loss: 0.0696 | Test Loss: 0.1456\n",
      "Epoch 21/200 | Train Loss: 0.0528 | Test Loss: 0.1220\n",
      "Epoch 31/200 | Train Loss: 0.0404 | Test Loss: 0.1385\n",
      "Epoch 41/200 | Train Loss: 0.0352 | Test Loss: 0.1338\n",
      "Early stopping at epoch 49\n",
      "\n",
      "=== Processing GOOG ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.4043 | Test Loss: 0.5614\n",
      "Epoch 11/200 | Train Loss: 0.0783 | Test Loss: 0.1536\n",
      "Epoch 21/200 | Train Loss: 0.0515 | Test Loss: 0.1691\n",
      "Epoch 31/200 | Train Loss: 0.0285 | Test Loss: 0.1591\n",
      "Early stopping at epoch 39\n",
      "Epoch 1/200 | Train Loss: 0.3765 | Test Loss: 0.5930\n",
      "Epoch 11/200 | Train Loss: 0.2716 | Test Loss: 0.5201\n",
      "Epoch 21/200 | Train Loss: 0.1347 | Test Loss: 0.5370\n",
      "Epoch 31/200 | Train Loss: 0.0800 | Test Loss: 0.5290\n",
      "Early stopping at epoch 34\n",
      "Epoch 1/200 | Train Loss: 0.4023 | Test Loss: 0.5898\n",
      "Epoch 11/200 | Train Loss: 0.0806 | Test Loss: 0.2032\n",
      "Epoch 21/200 | Train Loss: 0.0491 | Test Loss: 0.2007\n",
      "Early stopping at epoch 27\n",
      "Epoch 1/200 | Train Loss: 0.3685 | Test Loss: 0.5739\n",
      "Epoch 11/200 | Train Loss: 0.2813 | Test Loss: 0.5377\n",
      "Epoch 21/200 | Train Loss: 0.2231 | Test Loss: 0.5317\n",
      "Epoch 31/200 | Train Loss: 0.1831 | Test Loss: 0.5348\n",
      "Early stopping at epoch 37\n",
      "Epoch 1/200 | Train Loss: 0.3828 | Test Loss: 0.5931\n",
      "Epoch 11/200 | Train Loss: 0.0685 | Test Loss: 0.1833\n",
      "Epoch 21/200 | Train Loss: 0.0530 | Test Loss: 0.2160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.3737 | Test Loss: 0.5906\n",
      "Epoch 11/200 | Train Loss: 0.1252 | Test Loss: 0.2481\n",
      "Epoch 21/200 | Train Loss: 0.1006 | Test Loss: 0.2131\n",
      "Epoch 31/200 | Train Loss: 0.0829 | Test Loss: 0.3294\n",
      "Early stopping at epoch 39\n",
      "Epoch 1/200 | Train Loss: 0.3827 | Test Loss: 0.6034\n",
      "Epoch 11/200 | Train Loss: 0.1299 | Test Loss: 0.2347\n",
      "Epoch 21/200 | Train Loss: 0.1134 | Test Loss: 0.2120\n",
      "Epoch 31/200 | Train Loss: 0.1024 | Test Loss: 0.2377\n",
      "Early stopping at epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.3751 | Test Loss: 0.6095\n",
      "Epoch 11/200 | Train Loss: 0.0989 | Test Loss: 0.2189\n",
      "Epoch 21/200 | Train Loss: 0.0446 | Test Loss: 0.1799\n",
      "Early stopping at epoch 29\n",
      "Epoch 1/200 | Train Loss: 0.3917 | Test Loss: 0.5973\n",
      "Epoch 11/200 | Train Loss: 0.0731 | Test Loss: 0.2119\n",
      "Epoch 21/200 | Train Loss: 0.0499 | Test Loss: 0.1876\n",
      "Early stopping at epoch 29\n",
      "\n",
      "=== Processing META ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.3792 | Test Loss: 0.6239\n",
      "Epoch 11/200 | Train Loss: 0.0794 | Test Loss: 0.2393\n",
      "Epoch 21/200 | Train Loss: 0.0580 | Test Loss: 0.1441\n",
      "Epoch 31/200 | Train Loss: 0.0365 | Test Loss: 0.1675\n",
      "Epoch 41/200 | Train Loss: 0.0362 | Test Loss: 0.1964\n",
      "Early stopping at epoch 47\n",
      "Epoch 1/200 | Train Loss: 0.3829 | Test Loss: 0.7110\n",
      "Epoch 11/200 | Train Loss: 0.2399 | Test Loss: 0.6289\n",
      "Epoch 21/200 | Train Loss: 0.1306 | Test Loss: 0.5444\n",
      "Epoch 31/200 | Train Loss: 0.0841 | Test Loss: 0.5445\n",
      "Epoch 41/200 | Train Loss: 0.0641 | Test Loss: 0.6379\n",
      "Early stopping at epoch 48\n",
      "Epoch 1/200 | Train Loss: 0.3736 | Test Loss: 0.6957\n",
      "Epoch 11/200 | Train Loss: 0.0703 | Test Loss: 0.2340\n",
      "Epoch 21/200 | Train Loss: 0.0538 | Test Loss: 0.1496\n",
      "Epoch 31/200 | Train Loss: 0.0408 | Test Loss: 0.1664\n",
      "Epoch 41/200 | Train Loss: 0.0428 | Test Loss: 0.1778\n",
      "Epoch 51/200 | Train Loss: 0.0304 | Test Loss: 0.0968\n",
      "Epoch 61/200 | Train Loss: 0.0314 | Test Loss: 0.1385\n",
      "Early stopping at epoch 69\n",
      "Epoch 1/200 | Train Loss: 0.3950 | Test Loss: 0.6892\n",
      "Epoch 11/200 | Train Loss: 0.2493 | Test Loss: 0.6654\n",
      "Early stopping at epoch 18\n",
      "Epoch 1/200 | Train Loss: 0.3769 | Test Loss: 0.6410\n",
      "Epoch 11/200 | Train Loss: 0.0840 | Test Loss: 0.2283\n",
      "Epoch 21/200 | Train Loss: 0.0584 | Test Loss: 0.1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.3530 | Test Loss: 0.7028\n",
      "Epoch 11/200 | Train Loss: 0.1176 | Test Loss: 0.2406\n",
      "Epoch 21/200 | Train Loss: 0.0857 | Test Loss: 0.3842\n",
      "Early stopping at epoch 30\n",
      "Epoch 1/200 | Train Loss: 0.3996 | Test Loss: 0.6469\n",
      "Epoch 11/200 | Train Loss: 0.1161 | Test Loss: 0.2562\n",
      "Epoch 21/200 | Train Loss: 0.0950 | Test Loss: 0.3076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.4051 | Test Loss: 0.6594\n",
      "Epoch 11/200 | Train Loss: 0.0679 | Test Loss: 0.1943\n",
      "Epoch 21/200 | Train Loss: 0.0564 | Test Loss: 0.1443\n",
      "Epoch 31/200 | Train Loss: 0.0291 | Test Loss: 0.2538\n",
      "Early stopping at epoch 34\n",
      "Epoch 1/200 | Train Loss: 0.3603 | Test Loss: 0.6528\n",
      "Epoch 11/200 | Train Loss: 0.0907 | Test Loss: 0.2638\n",
      "Epoch 21/200 | Train Loss: 0.0492 | Test Loss: 0.1213\n",
      "Epoch 31/200 | Train Loss: 0.0416 | Test Loss: 0.1275\n",
      "Epoch 41/200 | Train Loss: 0.0397 | Test Loss: 0.1031\n",
      "Early stopping at epoch 44\n",
      "\n",
      "=== Processing NVDA ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.3691 | Test Loss: 0.5673\n",
      "Epoch 11/200 | Train Loss: 0.0762 | Test Loss: 0.2189\n",
      "Epoch 21/200 | Train Loss: 0.0512 | Test Loss: 0.1773\n",
      "Epoch 31/200 | Train Loss: 0.0322 | Test Loss: 0.1815\n",
      "Early stopping at epoch 33\n",
      "Epoch 1/200 | Train Loss: 0.3829 | Test Loss: 0.5721\n",
      "Epoch 11/200 | Train Loss: 0.2366 | Test Loss: 0.6525\n",
      "Early stopping at epoch 20\n",
      "Epoch 1/200 | Train Loss: 0.3949 | Test Loss: 0.6096\n",
      "Epoch 11/200 | Train Loss: 0.1108 | Test Loss: 0.1974\n",
      "Epoch 21/200 | Train Loss: 0.0645 | Test Loss: 0.1990\n",
      "Early stopping at epoch 30\n",
      "Epoch 1/200 | Train Loss: 0.3972 | Test Loss: 0.5800\n",
      "Epoch 11/200 | Train Loss: 0.2765 | Test Loss: 0.5381\n",
      "Epoch 21/200 | Train Loss: 0.1911 | Test Loss: 0.7150\n",
      "Early stopping at epoch 25\n",
      "Epoch 1/200 | Train Loss: 0.4265 | Test Loss: 0.6204\n",
      "Epoch 11/200 | Train Loss: 0.0689 | Test Loss: 0.1750\n",
      "Epoch 21/200 | Train Loss: 0.0477 | Test Loss: 0.1722\n",
      "Epoch 31/200 | Train Loss: 0.0360 | Test Loss: 0.1879\n",
      "Early stopping at epoch 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.3945 | Test Loss: 0.6158\n",
      "Epoch 11/200 | Train Loss: 0.1270 | Test Loss: 0.3055\n",
      "Epoch 21/200 | Train Loss: 0.0994 | Test Loss: 0.2482\n",
      "Epoch 31/200 | Train Loss: 0.0815 | Test Loss: 0.2653\n",
      "Early stopping at epoch 33\n",
      "Epoch 1/200 | Train Loss: 0.3898 | Test Loss: 0.6216\n",
      "Epoch 11/200 | Train Loss: 0.1225 | Test Loss: 0.3189\n",
      "Epoch 21/200 | Train Loss: 0.1109 | Test Loss: 0.2869\n",
      "Early stopping at epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.4282 | Test Loss: 0.5772\n",
      "Epoch 11/200 | Train Loss: 0.0704 | Test Loss: 0.2664\n",
      "Epoch 21/200 | Train Loss: 0.0479 | Test Loss: 0.1619\n",
      "Epoch 31/200 | Train Loss: 0.0304 | Test Loss: 0.1967\n",
      "Early stopping at epoch 34\n",
      "Epoch 1/200 | Train Loss: 0.4163 | Test Loss: 0.6187\n",
      "Epoch 11/200 | Train Loss: 0.0686 | Test Loss: 0.1798\n",
      "Epoch 21/200 | Train Loss: 0.0505 | Test Loss: 0.1789\n",
      "Epoch 31/200 | Train Loss: 0.0418 | Test Loss: 0.1615\n",
      "Epoch 41/200 | Train Loss: 0.0336 | Test Loss: 0.1721\n",
      "Early stopping at epoch 47\n",
      "\n",
      "=== Processing TSLA ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.3820 | Test Loss: 0.4666\n",
      "Epoch 11/200 | Train Loss: 0.0853 | Test Loss: 0.1640\n",
      "Epoch 21/200 | Train Loss: 0.0393 | Test Loss: 0.1390\n",
      "Epoch 31/200 | Train Loss: 0.0308 | Test Loss: 0.1394\n",
      "Epoch 41/200 | Train Loss: 0.0226 | Test Loss: 0.1202\n",
      "Epoch 51/200 | Train Loss: 0.0203 | Test Loss: 0.1230\n",
      "Early stopping at epoch 53\n",
      "Epoch 1/200 | Train Loss: 0.3866 | Test Loss: 0.4910\n",
      "Epoch 11/200 | Train Loss: 0.2434 | Test Loss: 0.4814\n",
      "Epoch 21/200 | Train Loss: 0.1335 | Test Loss: 0.6028\n",
      "Epoch 31/200 | Train Loss: 0.0795 | Test Loss: 0.5028\n",
      "Early stopping at epoch 38\n",
      "Epoch 1/200 | Train Loss: 0.4119 | Test Loss: 0.5028\n",
      "Epoch 11/200 | Train Loss: 0.0700 | Test Loss: 0.1612\n",
      "Epoch 21/200 | Train Loss: 0.0515 | Test Loss: 0.1414\n",
      "Epoch 31/200 | Train Loss: 0.0366 | Test Loss: 0.1448\n",
      "Early stopping at epoch 39\n",
      "Epoch 1/200 | Train Loss: 0.3842 | Test Loss: 0.4522\n",
      "Epoch 11/200 | Train Loss: 0.2928 | Test Loss: 0.4774\n",
      "Early stopping at epoch 15\n",
      "Epoch 1/200 | Train Loss: 0.4284 | Test Loss: 0.5129\n",
      "Epoch 11/200 | Train Loss: 0.0838 | Test Loss: 0.1651\n",
      "Epoch 21/200 | Train Loss: 0.0499 | Test Loss: 0.1568\n",
      "Epoch 31/200 | Train Loss: 0.0523 | Test Loss: 0.1007\n",
      "Epoch 41/200 | Train Loss: 0.0357 | Test Loss: 0.0918\n",
      "Epoch 51/200 | Train Loss: 0.0288 | Test Loss: 0.1003\n",
      "Epoch 61/200 | Train Loss: 0.0285 | Test Loss: 0.1021\n",
      "Early stopping at epoch 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.4152 | Test Loss: 0.5113\n",
      "Epoch 11/200 | Train Loss: 0.1184 | Test Loss: 0.1630\n",
      "Epoch 21/200 | Train Loss: 0.0892 | Test Loss: 0.1620\n",
      "Epoch 31/200 | Train Loss: 0.0781 | Test Loss: 0.1788\n",
      "Early stopping at epoch 35\n",
      "Epoch 1/200 | Train Loss: 0.4671 | Test Loss: 0.5135\n",
      "Epoch 11/200 | Train Loss: 0.1274 | Test Loss: 0.1881\n",
      "Epoch 21/200 | Train Loss: 0.1027 | Test Loss: 0.1826\n",
      "Epoch 31/200 | Train Loss: 0.0943 | Test Loss: 0.2051\n",
      "Early stopping at epoch 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 0.4144 | Test Loss: 0.5413\n",
      "Epoch 11/200 | Train Loss: 0.0653 | Test Loss: 0.1495\n",
      "Epoch 21/200 | Train Loss: 0.0425 | Test Loss: 0.1385\n",
      "Epoch 31/200 | Train Loss: 0.0277 | Test Loss: 0.1031\n",
      "Epoch 41/200 | Train Loss: 0.0225 | Test Loss: 0.1151\n",
      "Early stopping at epoch 49\n",
      "Epoch 1/200 | Train Loss: 0.3943 | Test Loss: 0.5049\n",
      "Epoch 11/200 | Train Loss: 0.0623 | Test Loss: 0.1345\n",
      "Epoch 21/200 | Train Loss: 0.0470 | Test Loss: 0.1499\n",
      "Early stopping at epoch 29\n",
      "\n",
      "=== Final Results ===\n",
      "\n",
      "AAPL Performance:\n",
      "Transformer:\n",
      "  RMSE: 0.5499 | MAE: 0.3245\n",
      "  Accuracy: 88.07% | Recall: 79.65%\n",
      "  Sharpe: -3.96 | Corr: 0.88\n",
      "  Precision: 0.97\n",
      "  F1: 0.87\n",
      "  AUC: 0.96\n",
      "  Confusion Matrix:\n",
      "[[309   9]\n",
      " [ 70 274]]\n",
      "Transformer_no_pe:\n",
      "  RMSE: 0.9128 | MAE: 0.6650\n",
      "  Accuracy: 69.18% | Recall: 64.24%\n",
      "  Sharpe: -2.19 | Corr: 0.49\n",
      "  Precision: 0.73\n",
      "  F1: 0.68\n",
      "  AUC: 0.76\n",
      "  Confusion Matrix:\n",
      "[[237  81]\n",
      " [123 221]]\n",
      "Informer:\n",
      "  RMSE: 0.4646 | MAE: 0.3171\n",
      "  Accuracy: 86.25% | Recall: 75.87%\n",
      "  Sharpe: -4.58 | Corr: 0.91\n",
      "  Precision: 0.97\n",
      "  F1: 0.85\n",
      "  AUC: 0.96\n",
      "  Confusion Matrix:\n",
      "[[310   8]\n",
      " [ 83 261]]\n",
      "Informer_no_pe:\n",
      "  RMSE: 1.0586 | MAE: 0.8068\n",
      "  Accuracy: 68.88% | Recall: 57.27%\n",
      "  Sharpe: -3.19 | Corr: 0.43\n",
      "  Precision: 0.77\n",
      "  F1: 0.66\n",
      "  AUC: 0.75\n",
      "  Confusion Matrix:\n",
      "[[259  59]\n",
      " [147 197]]\n",
      "Informer_no_probs:\n",
      "  RMSE: 0.4185 | MAE: 0.2888\n",
      "  Accuracy: 88.37% | Recall: 82.85%\n",
      "  Sharpe: -3.40 | Corr: 0.92\n",
      "  Precision: 0.94\n",
      "  F1: 0.88\n",
      "  AUC: 0.96\n",
      "  Confusion Matrix:\n",
      "[[300  18]\n",
      " [ 59 285]]\n",
      "Transformer_no_imf:\n",
      "  RMSE: 0.6264 | MAE: 0.4659\n",
      "  Accuracy: 80.97% | Recall: 70.06%\n",
      "  Sharpe: -5.59 | Corr: 0.82\n",
      "  Precision: 0.91\n",
      "  F1: 0.79\n",
      "  AUC: 0.91\n",
      "  Confusion Matrix:\n",
      "[[295  23]\n",
      " [103 241]]\n",
      "Informer_no_imf:\n",
      "  RMSE: 0.6121 | MAE: 0.4376\n",
      "  Accuracy: 80.51% | Recall: 80.23%\n",
      "  Sharpe: -1.62 | Corr: 0.81\n",
      "  Precision: 0.82\n",
      "  F1: 0.81\n",
      "  AUC: 0.89\n",
      "  Confusion Matrix:\n",
      "[[257  61]\n",
      " [ 68 276]]\n",
      "Transformer_no_tech:\n",
      "  RMSE: 0.5785 | MAE: 0.3540\n",
      "  Accuracy: 84.29% | Recall: 71.51%\n",
      "  Sharpe: -5.22 | Corr: 0.88\n",
      "  Precision: 0.98\n",
      "  F1: 0.83\n",
      "  AUC: 0.95\n",
      "  Confusion Matrix:\n",
      "[[312   6]\n",
      " [ 98 246]]\n",
      "Informer_no_tech:\n",
      "  RMSE: 0.5515 | MAE: 0.3853\n",
      "  Accuracy: 81.12% | Recall: 65.12%\n",
      "  Sharpe: -6.96 | Corr: 0.90\n",
      "  Precision: 0.98\n",
      "  F1: 0.78\n",
      "  AUC: 0.95\n",
      "  Confusion Matrix:\n",
      "[[313   5]\n",
      " [120 224]]\n",
      "\n",
      "MSFT Performance:\n",
      "Transformer:\n",
      "  RMSE: 0.5499 | MAE: 0.3790\n",
      "  Accuracy: 85.50% | Recall: 75.95%\n",
      "  Sharpe: -3.88 | Corr: 0.87\n",
      "  Precision: 0.95\n",
      "  F1: 0.84\n",
      "  AUC: 0.95\n",
      "  Confusion Matrix:\n",
      "[[307  14]\n",
      " [ 82 259]]\n",
      "Transformer_no_pe:\n",
      "  RMSE: 0.9405 | MAE: 0.7159\n",
      "  Accuracy: 68.43% | Recall: 70.38%\n",
      "  Sharpe: -0.77 | Corr: 0.49\n",
      "  Precision: 0.69\n",
      "  F1: 0.70\n",
      "  AUC: 0.74\n",
      "  Confusion Matrix:\n",
      "[[213 108]\n",
      " [101 240]]\n",
      "Informer:\n",
      "  RMSE: 0.5900 | MAE: 0.3636\n",
      "  Accuracy: 87.46% | Recall: 84.16%\n",
      "  Sharpe: -1.27 | Corr: 0.82\n",
      "  Precision: 0.91\n",
      "  F1: 0.87\n",
      "  AUC: 0.94\n",
      "  Confusion Matrix:\n",
      "[[292  29]\n",
      " [ 54 287]]\n",
      "Informer_no_pe:\n",
      "  RMSE: 1.0870 | MAE: 0.8465\n",
      "  Accuracy: 65.71% | Recall: 55.72%\n",
      "  Sharpe: -5.37 | Corr: 0.40\n",
      "  Precision: 0.71\n",
      "  F1: 0.63\n",
      "  AUC: 0.71\n",
      "  Confusion Matrix:\n",
      "[[245  76]\n",
      " [151 190]]\n",
      "Informer_no_probs:\n",
      "  RMSE: 0.6028 | MAE: 0.3704\n",
      "  Accuracy: 88.07% | Recall: 88.27%\n",
      "  Sharpe: 0.58 | Corr: 0.81\n",
      "  Precision: 0.89\n",
      "  F1: 0.88\n",
      "  AUC: 0.95\n",
      "  Confusion Matrix:\n",
      "[[282  39]\n",
      " [ 40 301]]\n",
      "Transformer_no_imf:\n",
      "  RMSE: 0.6637 | MAE: 0.5263\n",
      "  Accuracy: 77.79% | Recall: 65.10%\n",
      "  Sharpe: -4.54 | Corr: 0.81\n",
      "  Precision: 0.89\n",
      "  F1: 0.75\n",
      "  AUC: 0.90\n",
      "  Confusion Matrix:\n",
      "[[293  28]\n",
      " [119 222]]\n",
      "Informer_no_imf:\n",
      "  RMSE: 0.6258 | MAE: 0.4839\n",
      "  Accuracy: 81.12% | Recall: 91.50%\n",
      "  Sharpe: 3.56 | Corr: 0.82\n",
      "  Precision: 0.76\n",
      "  F1: 0.83\n",
      "  AUC: 0.91\n",
      "  Confusion Matrix:\n",
      "[[225  96]\n",
      " [ 29 312]]\n",
      "Transformer_no_tech:\n",
      "  RMSE: 0.5106 | MAE: 0.3266\n",
      "  Accuracy: 88.97% | Recall: 92.96%\n",
      "  Sharpe: 0.79 | Corr: 0.87\n",
      "  Precision: 0.87\n",
      "  F1: 0.90\n",
      "  AUC: 0.96\n",
      "  Confusion Matrix:\n",
      "[[272  49]\n",
      " [ 24 317]]\n",
      "Informer_no_tech:\n",
      "  RMSE: 0.5502 | MAE: 0.3696\n",
      "  Accuracy: 87.31% | Recall: 90.32%\n",
      "  Sharpe: 0.86 | Corr: 0.86\n",
      "  Precision: 0.86\n",
      "  F1: 0.88\n",
      "  AUC: 0.94\n",
      "  Confusion Matrix:\n",
      "[[270  51]\n",
      " [ 33 308]]\n",
      "\n",
      "AMZN Performance:\n",
      "Transformer:\n",
      "  RMSE: 0.4496 | MAE: 0.3190\n",
      "  Accuracy: 88.37% | Recall: 87.02%\n",
      "  Sharpe: -1.79 | Corr: 0.91\n",
      "  Precision: 0.90\n",
      "  F1: 0.88\n",
      "  AUC: 0.96\n",
      "  Confusion Matrix:\n",
      "[[290  33]\n",
      " [ 44 295]]\n",
      "Transformer_no_pe:\n",
      "  RMSE: 1.1142 | MAE: 0.8202\n",
      "  Accuracy: 68.73% | Recall: 81.42%\n",
      "  Sharpe: 2.06 | Corr: 0.48\n",
      "  Precision: 0.66\n",
      "  F1: 0.73\n",
      "  AUC: 0.73\n",
      "  Confusion Matrix:\n",
      "[[179 144]\n",
      " [ 63 276]]\n",
      "Informer:\n",
      "  RMSE: 0.4895 | MAE: 0.3387\n",
      "  Accuracy: 87.46% | Recall: 87.61%\n",
      "  Sharpe: -1.18 | Corr: 0.91\n",
      "  Precision: 0.88\n",
      "  F1: 0.88\n",
      "  AUC: 0.94\n",
      "  Confusion Matrix:\n",
      "[[282  41]\n",
      " [ 42 297]]\n",
      "Informer_no_pe:\n",
      "  RMSE: 0.9978 | MAE: 0.7625\n",
      "  Accuracy: 65.86% | Recall: 67.55%\n",
      "  Sharpe: -2.68 | Corr: 0.49\n",
      "  Precision: 0.66\n",
      "  F1: 0.67\n",
      "  AUC: 0.70\n",
      "  Confusion Matrix:\n",
      "[[207 116]\n",
      " [110 229]]\n",
      "Informer_no_probs:\n",
      "  RMSE: 0.5060 | MAE: 0.3461\n",
      "  Accuracy: 88.67% | Recall: 91.15%\n",
      "  Sharpe: -0.61 | Corr: 0.90\n",
      "  Precision: 0.87\n",
      "  F1: 0.89\n",
      "  AUC: 0.95\n",
      "  Confusion Matrix:\n",
      "[[278  45]\n",
      " [ 30 309]]\n",
      "Transformer_no_imf:\n",
      "  RMSE: 0.5854 | MAE: 0.4350\n",
      "  Accuracy: 80.82% | Recall: 78.76%\n",
      "  Sharpe: -1.06 | Corr: 0.84\n",
      "  Precision: 0.83\n",
      "  F1: 0.81\n",
      "  AUC: 0.90\n",
      "  Confusion Matrix:\n",
      "[[268  55]\n",
      " [ 72 267]]\n",
      "Informer_no_imf:\n",
      "  RMSE: 0.5716 | MAE: 0.4243\n",
      "  Accuracy: 82.02% | Recall: 87.02%\n",
      "  Sharpe: 0.96 | Corr: 0.86\n",
      "  Precision: 0.80\n",
      "  F1: 0.83\n",
      "  AUC: 0.91\n",
      "  Confusion Matrix:\n",
      "[[248  75]\n",
      " [ 44 295]]\n",
      "Transformer_no_tech:\n",
      "  RMSE: 0.4593 | MAE: 0.3238\n",
      "  Accuracy: 88.82% | Recall: 93.22%\n",
      "  Sharpe: 0.89 | Corr: 0.91\n",
      "  Precision: 0.86\n",
      "  F1: 0.90\n",
      "  AUC: 0.96\n",
      "  Confusion Matrix:\n",
      "[[272  51]\n",
      " [ 23 316]]\n",
      "Informer_no_tech:\n",
      "  RMSE: 0.4939 | MAE: 0.3503\n",
      "  Accuracy: 88.22% | Recall: 93.81%\n",
      "  Sharpe: 0.74 | Corr: 0.91\n",
      "  Precision: 0.85\n",
      "  F1: 0.89\n",
      "  AUC: 0.95\n",
      "  Confusion Matrix:\n",
      "[[266  57]\n",
      " [ 21 318]]\n",
      "\n",
      "GOOG Performance:\n",
      "Transformer:\n",
      "  RMSE: 0.5648 | MAE: 0.4000\n",
      "  Accuracy: 84.44% | Recall: 82.34%\n",
      "  Sharpe: -1.35 | Corr: 0.88\n",
      "  Precision: 0.86\n",
      "  F1: 0.84\n",
      "  AUC: 0.93\n",
      "  Confusion Matrix:\n",
      "[[284  44]\n",
      " [ 59 275]]\n",
      "Transformer_no_pe:\n",
      "  RMSE: 1.1464 | MAE: 0.8612\n",
      "  Accuracy: 62.39% | Recall: 55.39%\n",
      "  Sharpe: -2.22 | Corr: 0.43\n",
      "  Precision: 0.65\n",
      "  F1: 0.60\n",
      "  AUC: 0.68\n",
      "  Confusion Matrix:\n",
      "[[228 100]\n",
      " [149 185]]\n",
      "Informer:\n",
      "  RMSE: 0.6075 | MAE: 0.4390\n",
      "  Accuracy: 87.16% | Recall: 88.92%\n",
      "  Sharpe: 0.41 | Corr: 0.90\n",
      "  Precision: 0.86\n",
      "  F1: 0.87\n",
      "  AUC: 0.95\n",
      "  Confusion Matrix:\n",
      "[[280  48]\n",
      " [ 37 297]]\n",
      "Informer_no_pe:\n",
      "  RMSE: 1.0838 | MAE: 0.8129\n",
      "  Accuracy: 62.08% | Recall: 48.20%\n",
      "  Sharpe: -4.68 | Corr: 0.47\n",
      "  Precision: 0.67\n",
      "  F1: 0.56\n",
      "  AUC: 0.72\n",
      "  Confusion Matrix:\n",
      "[[250  78]\n",
      " [173 161]]\n",
      "Informer_no_probs:\n",
      "  RMSE: 0.5908 | MAE: 0.4139\n",
      "  Accuracy: 87.01% | Recall: 80.54%\n",
      "  Sharpe: -2.78 | Corr: 0.90\n",
      "  Precision: 0.93\n",
      "  F1: 0.86\n",
      "  AUC: 0.96\n",
      "  Confusion Matrix:\n",
      "[[307  21]\n",
      " [ 65 269]]\n",
      "Transformer_no_imf:\n",
      "  RMSE: 0.8140 | MAE: 0.5963\n",
      "  Accuracy: 76.59% | Recall: 58.98%\n",
      "  Sharpe: -6.29 | Corr: 0.79\n",
      "  Precision: 0.92\n",
      "  F1: 0.72\n",
      "  AUC: 0.88\n",
      "  Confusion Matrix:\n",
      "[[310  18]\n",
      " [137 197]]\n",
      "Informer_no_imf:\n",
      "  RMSE: 0.6666 | MAE: 0.5110\n",
      "  Accuracy: 80.51% | Recall: 80.24%\n",
      "  Sharpe: -0.04 | Corr: 0.84\n",
      "  Precision: 0.81\n",
      "  F1: 0.81\n",
      "  AUC: 0.90\n",
      "  Confusion Matrix:\n",
      "[[265  63]\n",
      " [ 66 268]]\n",
      "Transformer_no_tech:\n",
      "  RMSE: 0.5283 | MAE: 0.3672\n",
      "  Accuracy: 86.86% | Recall: 83.53%\n",
      "  Sharpe: -1.69 | Corr: 0.90\n",
      "  Precision: 0.90\n",
      "  F1: 0.87\n",
      "  AUC: 0.95\n",
      "  Confusion Matrix:\n",
      "[[296  32]\n",
      " [ 55 279]]\n",
      "Informer_no_tech:\n",
      "  RMSE: 0.5992 | MAE: 0.4338\n",
      "  Accuracy: 85.35% | Recall: 77.25%\n",
      "  Sharpe: -1.96 | Corr: 0.89\n",
      "  Precision: 0.92\n",
      "  F1: 0.84\n",
      "  AUC: 0.95\n",
      "  Confusion Matrix:\n",
      "[[307  21]\n",
      " [ 76 258]]\n",
      "\n",
      "META Performance:\n",
      "Transformer:\n",
      "  RMSE: 0.5021 | MAE: 0.3245\n",
      "  Accuracy: 88.80% | Recall: 88.77%\n",
      "  Sharpe: 0.69 | Corr: 0.90\n",
      "  Precision: 0.92\n",
      "  F1: 0.90\n",
      "  AUC: 0.94\n",
      "  Confusion Matrix:\n",
      "[[239  30]\n",
      " [ 42 332]]\n",
      "Transformer_no_pe:\n",
      "  RMSE: 1.0199 | MAE: 0.7134\n",
      "  Accuracy: 69.52% | Recall: 63.37%\n",
      "  Sharpe: -1.08 | Corr: 0.48\n",
      "  Precision: 0.80\n",
      "  F1: 0.71\n",
      "  AUC: 0.76\n",
      "  Confusion Matrix:\n",
      "[[210  59]\n",
      " [137 237]]\n",
      "Informer:\n",
      "  RMSE: 0.5140 | MAE: 0.3666\n",
      "  Accuracy: 87.09% | Recall: 82.35%\n",
      "  Sharpe: -0.84 | Corr: 0.92\n",
      "  Precision: 0.95\n",
      "  F1: 0.88\n",
      "  AUC: 0.95\n",
      "  Confusion Matrix:\n",
      "[[252  17]\n",
      " [ 66 308]]\n",
      "Informer_no_pe:\n",
      "  RMSE: 1.1087 | MAE: 0.7953\n",
      "  Accuracy: 59.10% | Recall: 54.28%\n",
      "  Sharpe: -0.72 | Corr: 0.33\n",
      "  Precision: 0.69\n",
      "  F1: 0.61\n",
      "  AUC: 0.65\n",
      "  Confusion Matrix:\n",
      "[[177  92]\n",
      " [171 203]]\n",
      "Informer_no_probs:\n",
      "  RMSE: 0.4432 | MAE: 0.3190\n",
      "  Accuracy: 88.02% | Recall: 86.63%\n",
      "  Sharpe: 0.94 | Corr: 0.93\n",
      "  Precision: 0.92\n",
      "  F1: 0.89\n",
      "  AUC: 0.95\n",
      "  Confusion Matrix:\n",
      "[[242  27]\n",
      " [ 50 324]]\n",
      "Transformer_no_imf:\n",
      "  RMSE: 0.6293 | MAE: 0.4457\n",
      "  Accuracy: 81.34% | Recall: 75.40%\n",
      "  Sharpe: -1.41 | Corr: 0.86\n",
      "  Precision: 0.91\n",
      "  F1: 0.82\n",
      "  AUC: 0.91\n",
      "  Confusion Matrix:\n",
      "[[241  28]\n",
      " [ 92 282]]\n",
      "Informer_no_imf:\n",
      "  RMSE: 0.6452 | MAE: 0.4990\n",
      "  Accuracy: 75.74% | Recall: 62.83%\n",
      "  Sharpe: -4.07 | Corr: 0.88\n",
      "  Precision: 0.93\n",
      "  F1: 0.75\n",
      "  AUC: 0.91\n",
      "  Confusion Matrix:\n",
      "[[252  17]\n",
      " [139 235]]\n",
      "Transformer_no_tech:\n",
      "  RMSE: 0.4439 | MAE: 0.3027\n",
      "  Accuracy: 89.58% | Recall: 93.58%\n",
      "  Sharpe: 3.18 | Corr: 0.93\n",
      "  Precision: 0.89\n",
      "  F1: 0.91\n",
      "  AUC: 0.96\n",
      "  Confusion Matrix:\n",
      "[[226  43]\n",
      " [ 24 350]]\n",
      "Informer_no_tech:\n",
      "  RMSE: 0.4599 | MAE: 0.3176\n",
      "  Accuracy: 89.11% | Recall: 90.37%\n",
      "  Sharpe: 2.38 | Corr: 0.94\n",
      "  Precision: 0.91\n",
      "  F1: 0.91\n",
      "  AUC: 0.96\n",
      "  Confusion Matrix:\n",
      "[[235  34]\n",
      " [ 36 338]]\n",
      "\n",
      "NVDA Performance:\n",
      "Transformer:\n",
      "  RMSE: 0.5636 | MAE: 0.4279\n",
      "  Accuracy: 87.46% | Recall: 84.66%\n",
      "  Sharpe: -0.94 | Corr: 0.90\n",
      "  Precision: 0.92\n",
      "  F1: 0.88\n",
      "  AUC: 0.94\n",
      "  Confusion Matrix:\n",
      "[[270  27]\n",
      " [ 56 309]]\n",
      "Transformer_no_pe:\n",
      "  RMSE: 1.1898 | MAE: 0.9589\n",
      "  Accuracy: 62.24% | Recall: 68.49%\n",
      "  Sharpe: 2.22 | Corr: 0.38\n",
      "  Precision: 0.65\n",
      "  F1: 0.67\n",
      "  AUC: 0.68\n",
      "  Confusion Matrix:\n",
      "[[162 135]\n",
      " [115 250]]\n",
      "Informer:\n",
      "  RMSE: 0.5646 | MAE: 0.4152\n",
      "  Accuracy: 85.20% | Recall: 81.10%\n",
      "  Sharpe: -0.45 | Corr: 0.89\n",
      "  Precision: 0.91\n",
      "  F1: 0.86\n",
      "  AUC: 0.94\n",
      "  Confusion Matrix:\n",
      "[[268  29]\n",
      " [ 69 296]]\n",
      "Informer_no_pe:\n",
      "  RMSE: 1.4298 | MAE: 1.0872\n",
      "  Accuracy: 64.95% | Recall: 69.59%\n",
      "  Sharpe: -0.90 | Corr: 0.38\n",
      "  Precision: 0.68\n",
      "  F1: 0.69\n",
      "  AUC: 0.69\n",
      "  Confusion Matrix:\n",
      "[[176 121]\n",
      " [111 254]]\n",
      "Informer_no_probs:\n",
      "  RMSE: 0.5152 | MAE: 0.3758\n",
      "  Accuracy: 86.71% | Recall: 90.14%\n",
      "  Sharpe: 2.72 | Corr: 0.90\n",
      "  Precision: 0.86\n",
      "  F1: 0.88\n",
      "  AUC: 0.94\n",
      "  Confusion Matrix:\n",
      "[[245  52]\n",
      " [ 36 329]]\n",
      "Transformer_no_imf:\n",
      "  RMSE: 0.6546 | MAE: 0.5042\n",
      "  Accuracy: 83.69% | Recall: 83.56%\n",
      "  Sharpe: 0.63 | Corr: 0.83\n",
      "  Precision: 0.86\n",
      "  F1: 0.85\n",
      "  AUC: 0.91\n",
      "  Confusion Matrix:\n",
      "[[249  48]\n",
      " [ 60 305]]\n",
      "Informer_no_imf:\n",
      "  RMSE: 0.6780 | MAE: 0.5255\n",
      "  Accuracy: 81.42% | Recall: 79.73%\n",
      "  Sharpe: 0.21 | Corr: 0.82\n",
      "  Precision: 0.86\n",
      "  F1: 0.83\n",
      "  AUC: 0.90\n",
      "  Confusion Matrix:\n",
      "[[248  49]\n",
      " [ 74 291]]\n",
      "Transformer_no_tech:\n",
      "  RMSE: 0.5189 | MAE: 0.3881\n",
      "  Accuracy: 84.59% | Recall: 80.55%\n",
      "  Sharpe: -0.43 | Corr: 0.90\n",
      "  Precision: 0.90\n",
      "  F1: 0.85\n",
      "  AUC: 0.94\n",
      "  Confusion Matrix:\n",
      "[[266  31]\n",
      " [ 71 294]]\n",
      "Informer_no_tech:\n",
      "  RMSE: 0.5446 | MAE: 0.4143\n",
      "  Accuracy: 86.56% | Recall: 89.59%\n",
      "  Sharpe: 1.51 | Corr: 0.91\n",
      "  Precision: 0.87\n",
      "  F1: 0.88\n",
      "  AUC: 0.94\n",
      "  Confusion Matrix:\n",
      "[[246  51]\n",
      " [ 38 327]]\n",
      "\n",
      "TSLA Performance:\n",
      "Transformer:\n",
      "  RMSE: 0.5020 | MAE: 0.3946\n",
      "  Accuracy: 86.25% | Recall: 80.00%\n",
      "  Sharpe: -1.80 | Corr: 0.91\n",
      "  Precision: 0.89\n",
      "  F1: 0.84\n",
      "  AUC: 0.94\n",
      "  Confusion Matrix:\n",
      "[[331  31]\n",
      " [ 60 240]]\n",
      "Transformer_no_pe:\n",
      "  RMSE: 1.0492 | MAE: 0.8135\n",
      "  Accuracy: 69.64% | Recall: 63.33%\n",
      "  Sharpe: -2.89 | Corr: 0.57\n",
      "  Precision: 0.68\n",
      "  F1: 0.65\n",
      "  AUC: 0.77\n",
      "  Confusion Matrix:\n",
      "[[271  91]\n",
      " [110 190]]\n",
      "Informer:\n",
      "  RMSE: 0.4568 | MAE: 0.3513\n",
      "  Accuracy: 88.07% | Recall: 85.67%\n",
      "  Sharpe: -1.39 | Corr: 0.93\n",
      "  Precision: 0.88\n",
      "  F1: 0.87\n",
      "  AUC: 0.95\n",
      "  Confusion Matrix:\n",
      "[[326  36]\n",
      " [ 43 257]]\n",
      "Informer_no_pe:\n",
      "  RMSE: 1.1764 | MAE: 0.9404\n",
      "  Accuracy: 66.31% | Recall: 65.67%\n",
      "  Sharpe: 3.42 | Corr: 0.48\n",
      "  Precision: 0.62\n",
      "  F1: 0.64\n",
      "  AUC: 0.74\n",
      "  Confusion Matrix:\n",
      "[[242 120]\n",
      " [103 197]]\n",
      "Informer_no_probs:\n",
      "  RMSE: 0.4218 | MAE: 0.3211\n",
      "  Accuracy: 87.92% | Recall: 82.67%\n",
      "  Sharpe: -3.12 | Corr: 0.93\n",
      "  Precision: 0.90\n",
      "  F1: 0.86\n",
      "  AUC: 0.96\n",
      "  Confusion Matrix:\n",
      "[[334  28]\n",
      " [ 52 248]]\n",
      "Transformer_no_imf:\n",
      "  RMSE: 0.5867 | MAE: 0.4492\n",
      "  Accuracy: 81.72% | Recall: 88.00%\n",
      "  Sharpe: 1.20 | Corr: 0.86\n",
      "  Precision: 0.76\n",
      "  F1: 0.81\n",
      "  AUC: 0.92\n",
      "  Confusion Matrix:\n",
      "[[277  85]\n",
      " [ 36 264]]\n",
      "Informer_no_imf:\n",
      "  RMSE: 0.6504 | MAE: 0.4917\n",
      "  Accuracy: 81.27% | Recall: 76.67%\n",
      "  Sharpe: -2.03 | Corr: 0.84\n",
      "  Precision: 0.81\n",
      "  F1: 0.79\n",
      "  AUC: 0.90\n",
      "  Confusion Matrix:\n",
      "[[308  54]\n",
      " [ 70 230]]\n",
      "Transformer_no_tech:\n",
      "  RMSE: 0.4039 | MAE: 0.3030\n",
      "  Accuracy: 88.37% | Recall: 82.67%\n",
      "  Sharpe: -2.44 | Corr: 0.93\n",
      "  Precision: 0.91\n",
      "  F1: 0.87\n",
      "  AUC: 0.96\n",
      "  Confusion Matrix:\n",
      "[[337  25]\n",
      " [ 52 248]]\n",
      "Informer_no_tech:\n",
      "  RMSE: 0.4671 | MAE: 0.3528\n",
      "  Accuracy: 89.27% | Recall: 89.67%\n",
      "  Sharpe: -0.83 | Corr: 0.94\n",
      "  Precision: 0.87\n",
      "  F1: 0.88\n",
      "  AUC: 0.96\n",
      "  Confusion Matrix:\n",
      "[[322  40]\n",
      " [ 31 269]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score, mean_squared_error, mean_absolute_error, f1_score, confusion_matrix, precision_score, roc_curve, auc\n",
    "from scipy.signal import savgol_filter\n",
    "from tqdm import tqdm\n",
    "from PyEMD import EEMD\n",
    "import time\n",
    "\n",
    "plt.rcParams['font.family'] = 'SimHei'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class Config:\n",
    "    symbols = ['AAPL', 'MSFT', 'AMZN', 'GOOG', 'META', 'NVDA', 'TSLA']\n",
    "    start_date = '2012-01-01'\n",
    "    end_date = pd.Timestamp.now().strftime('%Y-%m-%d')\n",
    "    seq_length = 20\n",
    "    train_ratio = 0.8\n",
    "    batch_size = 128\n",
    "    epochs = 200\n",
    "    lr = 5e-4\n",
    "    weight_decay = 1e-4\n",
    "    dropout = 0.2\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model_config = {\n",
    "        'Transformer': {'d_model': 128, 'nhead': 8, 'num_layers': 3},\n",
    "        'Informer': {'d_model': 128, 'nhead': 8, 'num_layers': 3}\n",
    "    }\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def get_data(self, symbol):\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                df = yf.download(symbol, start=Config.start_date, end=Config.end_date)\n",
    "                return df[['Open', 'High', 'Low', 'Close', 'Volume']].dropna()\n",
    "            except yf.YFRateLimitError:\n",
    "                print(f\"Rate limit reached for {symbol}. Retrying in 60 seconds...\")\n",
    "                time.sleep(60)\n",
    "        print(f\"Failed to download data for {symbol} after {max_retries} attempts.\")\n",
    "        return None\n",
    "\n",
    "    def compute_rsi(self, prices, period=14):\n",
    "        delta = prices.diff()\n",
    "        gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
    "        loss = -delta.where(delta < 0, 0).rolling(window=period).mean()\n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "\n",
    "    def compute_macd(self, prices, fast=12, slow=26, signal=9):\n",
    "        ema_fast = prices.ewm(span=fast, adjust=False).mean()\n",
    "        ema_slow = prices.ewm(span=slow, adjust=False).mean()\n",
    "        macd = ema_fast - ema_slow\n",
    "        signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
    "        return macd, signal_line\n",
    "\n",
    "    def add_features(self, df):\n",
    "        df['LogReturn'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "        df['MA5'] = df['Close'].rolling(5).mean()\n",
    "        df['MA20'] = df['Close'].rolling(20).mean()\n",
    "        df['RSI'] = self.compute_rsi(df['Close'])\n",
    "        macd, signal = self.compute_macd(df['Close'])\n",
    "        df['MACD'] = macd\n",
    "        df['MACD_Signal'] = signal\n",
    "        df['DayOfWeek'] = df.index.dayofweek\n",
    "        df['Month'] = df.index.month - 1\n",
    "        return df.dropna()\n",
    "\n",
    "    def decompose_with_eemd(self, series):\n",
    "        eemd = EEMD()\n",
    "        T = np.arange(len(series))\n",
    "        imfs = eemd.emd(series, T)\n",
    "        return imfs\n",
    "\n",
    "    def plot_eemd_decomposition(self, symbol, series, imfs):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(len(imfs) + 1, 1, 1)\n",
    "        plt.plot(series, label='series')\n",
    "        plt.title(f'{symbol} EEMD')\n",
    "        plt.legend()\n",
    "\n",
    "        for i, imf in enumerate(imfs):\n",
    "            plt.subplot(len(imfs) + 1, 1, i + 2)\n",
    "            plt.plot(imf, label=f'IMF_{i}')\n",
    "            plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{symbol}_eemd_decomposition.png')\n",
    "        plt.close()\n",
    "\n",
    "    def process_data(self, symbol, use_imf=True, use_tech_indicators=True):\n",
    "        df = self.get_data(symbol)\n",
    "        if df is None:\n",
    "            return None, None\n",
    "        df = self.add_features(df)\n",
    "        window_length = min(11, len(df['LogReturn']))\n",
    "        df['LogReturn'] = savgol_filter(df['LogReturn'], window_length, 2)\n",
    "\n",
    "        log_return_series = df['LogReturn'].values\n",
    "        imfs = self.decompose_with_eemd(log_return_series)\n",
    "        self.plot_eemd_decomposition(symbol, log_return_series, imfs)\n",
    "\n",
    "        if use_imf:\n",
    "            for i, imf in enumerate(imfs):\n",
    "                df[f'IMF_{i}'] = imf\n",
    "\n",
    "        base_features = ['LogReturn', 'MA5', 'MA20', 'DayOfWeek', 'Month']\n",
    "        tech_features = ['RSI', 'MACD', 'MACD_Signal']\n",
    "        features = base_features.copy()\n",
    "        if use_tech_indicators:\n",
    "            features.extend(tech_features)\n",
    "        if use_imf:\n",
    "            for i in range(len(imfs)):\n",
    "                features.append(f'IMF_{i}')\n",
    "        target = 'LogReturn'\n",
    "        df[features] = self.scaler.fit_transform(df[features])\n",
    "\n",
    "        X, y = [], []\n",
    "        for i in range(len(df) - Config.seq_length):\n",
    "            X.append(df[features].values[i:i + Config.seq_length])\n",
    "            y.append(df[target].values[i + Config.seq_length])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, d_model, nhead, num_layers, dropout, use_positional_encoding=True):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_size, d_model)\n",
    "        self.use_positional_encoding = use_positional_encoding\n",
    "        if use_positional_encoding:\n",
    "            self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, dim_feedforward=d_model * 4, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model, nhead, dim_feedforward=d_model * 4, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = nn.Linear(d_model, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        if self.use_positional_encoding:\n",
    "            x = self.pos_encoder(x)\n",
    "        memory = self.transformer_encoder(x)\n",
    "\n",
    "        tgt = x\n",
    "        if self.use_positional_encoding:\n",
    "            tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer_decoder(tgt, memory)\n",
    "\n",
    "        output = self.norm(output.mean(dim=1))\n",
    "        output = self.dropout(output)\n",
    "        return self.decoder(output)\n",
    "\n",
    "\n",
    "class ProbSparseAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, factor):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, nhead, dropout=Config.dropout, batch_first=True)\n",
    "        self.factor = factor\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        attn_output, _ = self.attention(q, k, v)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class InformerModel(nn.Module):\n",
    "    def __init__(self, input_size, d_model, nhead, num_layers, dropout, factor=5, use_positional_encoding=True, use_probs_sparse_attn=True):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_size, d_model)\n",
    "        self.use_positional_encoding = use_positional_encoding\n",
    "        if use_positional_encoding:\n",
    "            self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.use_probs_sparse_attn = use_probs_sparse_attn\n",
    "        if use_probs_sparse_attn:\n",
    "            self.attn_layers = nn.ModuleList([\n",
    "                ProbSparseAttention(d_model, nhead, factor) for _ in range(num_layers)\n",
    "            ])\n",
    "        else:\n",
    "            self.attn_layers = nn.ModuleList([\n",
    "                nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True) for _ in range(num_layers)\n",
    "            ])\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model, nhead, dim_feedforward=d_model * 4, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = nn.Linear(d_model, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        if self.use_positional_encoding:\n",
    "            x = self.pos_encoder(x)\n",
    "        for attn in self.attn_layers:\n",
    "            if isinstance(attn, ProbSparseAttention):\n",
    "                x = attn(x, x, x)\n",
    "            else:\n",
    "                #  torch.Tensor \n",
    "                x, _ = attn(x, x, x)\n",
    "        memory = x\n",
    "\n",
    "        tgt = x\n",
    "        if self.use_positional_encoding:\n",
    "            tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer_decoder(tgt, memory)\n",
    "\n",
    "        output = self.norm(output.mean(dim=1))\n",
    "        output = self.dropout(output)\n",
    "        return self.decoder(output)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, model_name):\n",
    "        self.model = model.to(Config.device)\n",
    "        self.model_name = model_name\n",
    "        self.criterion = nn.HuberLoss(delta=1.0)\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=Config.lr, weight_decay=Config.weight_decay)\n",
    "        self.warmup_scheduler = optim.lr_scheduler.LinearLR(self.optimizer, start_factor=0.1, total_iters=10)\n",
    "        self.cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=Config.epochs - 10)\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop_counter = 0\n",
    "\n",
    "    def train_epoch(self, train_loader, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(Config.device), y.to(Config.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(X)\n",
    "            loss = self.criterion(outputs.squeeze(), y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if epoch < 10:\n",
    "            self.warmup_scheduler.step()\n",
    "        else:\n",
    "            self.cosine_scheduler.step()\n",
    "        return total_loss / len(train_loader)\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        preds, truths = [], []\n",
    "        with torch.no_grad():\n",
    "            for X, y in test_loader:\n",
    "                X, y = X.to(Config.device), y.to(Config.device)\n",
    "                outputs = self.model(X)\n",
    "                loss = self.criterion(outputs.squeeze(), y)\n",
    "                total_loss += loss.item()\n",
    "                preds.extend(outputs.cpu().numpy().flatten())\n",
    "                truths.extend(y.cpu().numpy().flatten())\n",
    "        return total_loss / len(test_loader), np.array(preds), np.array(truths)\n",
    "\n",
    "    def train(self, train_loader, test_loader):\n",
    "        for epoch in range(Config.epochs):\n",
    "            train_loss = self.train_epoch(train_loader, epoch)\n",
    "            test_loss, preds, truths = self.evaluate(test_loader)\n",
    "            if test_loss < self.best_loss:\n",
    "                self.best_loss = test_loss\n",
    "                torch.save(self.model.state_dict(), f'best_{self.model_name}.pth')\n",
    "                self.early_stop_counter = 0\n",
    "            else:\n",
    "                self.early_stop_counter += 1\n",
    "                if self.early_stop_counter >= 15:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{Config.epochs} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "        self.model.load_state_dict(torch.load(f'best_{self.model_name}.pth'))\n",
    "        return preds, truths\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    @staticmethod\n",
    "    def calculate_metrics(preds, truths):\n",
    "        pred_dir = (preds > 0).astype(int)\n",
    "        true_dir = (truths > 0).astype(int)\n",
    "        fpr, tpr, thresholds = roc_curve(true_dir, preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        metrics = {\n",
    "            'RMSE': np.sqrt(mean_squared_error(truths, preds)),\n",
    "            'MAE': mean_absolute_error(truths, preds),\n",
    "            'Accuracy': accuracy_score(true_dir, pred_dir),\n",
    "            'Recall': recall_score(true_dir, pred_dir),\n",
    "            'Sharpe': np.mean(preds) / np.std(preds) * np.sqrt(252) if np.std(preds) != 0 else 0,\n",
    "            'Correlation': np.corrcoef(truths, preds)[0, 1],\n",
    "            'F1': f1_score(true_dir, pred_dir),\n",
    "            'ConfusionMatrix': confusion_matrix(true_dir, pred_dir),\n",
    "            'Precision': precision_score(true_dir, pred_dir),\n",
    "            'AUC': roc_auc\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    processor = DataProcessor()\n",
    "    results = {}\n",
    "    for symbol in Config.symbols:\n",
    "        print(f\"\\n=== Processing {symbol} ===\")\n",
    "\n",
    "        # \n",
    "        X, y = processor.process_data(symbol, use_imf=True, use_tech_indicators=True)\n",
    "        if X is None or y is None:\n",
    "            continue\n",
    "        train_size = int(len(X) * Config.train_ratio)\n",
    "        X_train, X_test = X[:train_size], X[train_size:]\n",
    "        y_train, y_test = y[:train_size], y[train_size:]\n",
    "        train_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "        test_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=Config.batch_size)\n",
    "        model_results = {}\n",
    "        input_size = X_train.shape[2]\n",
    "\n",
    "        # Transformer\n",
    "        transformer = TransformerModel(input_size, **Config.model_config['Transformer'], dropout=Config.dropout, use_positional_encoding=True)\n",
    "        trans_trainer = Trainer(transformer, 'Transformer')\n",
    "        trans_preds, trans_truths = trans_trainer.train(train_loader, test_loader)\n",
    "        model_results['Transformer'] = Evaluator.calculate_metrics(trans_preds, trans_truths)\n",
    "\n",
    "        # Transformer\n",
    "        transformer_no_pe = TransformerModel(input_size, **Config.model_config['Transformer'], dropout=Config.dropout, use_positional_encoding=False)\n",
    "        trans_no_pe_trainer = Trainer(transformer_no_pe, 'Transformer_no_pe')\n",
    "        trans_no_pe_preds, trans_no_pe_truths = trans_no_pe_trainer.train(train_loader, test_loader)\n",
    "        model_results['Transformer_no_pe'] = Evaluator.calculate_metrics(trans_no_pe_preds, trans_no_pe_truths)\n",
    "\n",
    "        # Informer\n",
    "        informer = InformerModel(input_size, **Config.model_config['Informer'], dropout=Config.dropout, use_positional_encoding=True, use_probs_sparse_attn=True)\n",
    "        informer_trainer = Trainer(informer, 'Informer')\n",
    "        informer_preds, informer_truths = informer_trainer.train(train_loader, test_loader)\n",
    "        model_results['Informer'] = Evaluator.calculate_metrics(informer_preds, informer_truths)\n",
    "\n",
    "        # Informer\n",
    "        informer_no_pe = InformerModel(input_size, **Config.model_config['Informer'], dropout=Config.dropout, use_positional_encoding=False, use_probs_sparse_attn=True)\n",
    "        informer_no_pe_trainer = Trainer(informer_no_pe, 'Informer_no_pe')\n",
    "        informer_no_pe_preds, informer_no_pe_truths = informer_no_pe_trainer.train(train_loader, test_loader)\n",
    "        model_results['Informer_no_pe'] = Evaluator.calculate_metrics(informer_no_pe_preds, informer_no_pe_truths)\n",
    "\n",
    "        # ProbSparseAttentionInformer\n",
    "        informer_no_probs = InformerModel(input_size, **Config.model_config['Informer'], dropout=Config.dropout, use_positional_encoding=True, use_probs_sparse_attn=False)\n",
    "        informer_no_probs_trainer = Trainer(informer_no_probs, 'Informer_no_probs')\n",
    "        informer_no_probs_preds, informer_no_probs_truths = informer_no_probs_trainer.train(train_loader, test_loader)\n",
    "        model_results['Informer_no_probs'] = Evaluator.calculate_metrics(informer_no_probs_preds, informer_no_probs_truths)\n",
    "\n",
    "        # IMF\n",
    "        X_no_imf, y_no_imf = processor.process_data(symbol, use_imf=False, use_tech_indicators=True)\n",
    "        train_size_no_imf = int(len(X_no_imf) * Config.train_ratio)\n",
    "        X_train_no_imf, X_test_no_imf = X_no_imf[:train_size_no_imf], X_no_imf[train_size_no_imf:]\n",
    "        y_train_no_imf, y_test_no_imf = y_no_imf[:train_size_no_imf], y_no_imf[train_size_no_imf:]\n",
    "        train_dataset_no_imf = torch.utils.data.TensorDataset(torch.FloatTensor(X_train_no_imf), torch.FloatTensor(y_train_no_imf))\n",
    "        test_dataset_no_imf = torch.utils.data.TensorDataset(torch.FloatTensor(X_test_no_imf), torch.FloatTensor(y_test_no_imf))\n",
    "        train_loader_no_imf = DataLoader(train_dataset_no_imf, batch_size=Config.batch_size, shuffle=True)\n",
    "        test_loader_no_imf = DataLoader(test_dataset_no_imf, batch_size=Config.batch_size)\n",
    "        input_size_no_imf = X_train_no_imf.shape[2]\n",
    "\n",
    "        transformer_no_imf = TransformerModel(input_size_no_imf, **Config.model_config['Transformer'], dropout=Config.dropout, use_positional_encoding=True)\n",
    "        trans_no_imf_trainer = Trainer(transformer_no_imf, 'Transformer_no_imf')\n",
    "        trans_no_imf_preds, trans_no_imf_truths = trans_no_imf_trainer.train(train_loader_no_imf, test_loader_no_imf)\n",
    "        model_results['Transformer_no_imf'] = Evaluator.calculate_metrics(trans_no_imf_preds, trans_no_imf_truths)\n",
    "\n",
    "        informer_no_imf = InformerModel(input_size_no_imf, **Config.model_config['Informer'], dropout=Config.dropout, use_positional_encoding=True, use_probs_sparse_attn=True)\n",
    "        informer_no_imf_trainer = Trainer(informer_no_imf, 'Informer_no_imf')\n",
    "        informer_no_imf_preds, informer_no_imf_truths = informer_no_imf_trainer.train(train_loader_no_imf, test_loader_no_imf)\n",
    "        model_results['Informer_no_imf'] = Evaluator.calculate_metrics(informer_no_imf_preds, informer_no_imf_truths)\n",
    "\n",
    "        # \n",
    "        X_no_tech, y_no_tech = processor.process_data(symbol, use_imf=True, use_tech_indicators=False)\n",
    "        train_size_no_tech = int(len(X_no_tech) * Config.train_ratio)\n",
    "        X_train_no_tech, X_test_no_tech = X_no_tech[:train_size_no_tech], X_no_tech[train_size_no_tech:]\n",
    "        y_train_no_tech, y_test_no_tech = y_no_tech[:train_size_no_tech], y_no_tech[train_size_no_tech:]\n",
    "        train_dataset_no_tech = torch.utils.data.TensorDataset(torch.FloatTensor(X_train_no_tech), torch.FloatTensor(y_train_no_tech))\n",
    "        test_dataset_no_tech = torch.utils.data.TensorDataset(torch.FloatTensor(X_test_no_tech), torch.FloatTensor(y_test_no_tech))\n",
    "        train_loader_no_tech = DataLoader(train_dataset_no_tech, batch_size=Config.batch_size, shuffle=True)\n",
    "        test_loader_no_tech = DataLoader(test_dataset_no_tech, batch_size=Config.batch_size)\n",
    "        input_size_no_tech = X_train_no_tech.shape[2]\n",
    "\n",
    "        transformer_no_tech = TransformerModel(input_size_no_tech, **Config.model_config['Transformer'], dropout=Config.dropout, use_positional_encoding=True)\n",
    "        trans_no_tech_trainer = Trainer(transformer_no_tech, 'Transformer_no_tech')\n",
    "        trans_no_tech_preds, trans_no_tech_truths = trans_no_tech_trainer.train(train_loader_no_tech, test_loader_no_tech)\n",
    "        model_results['Transformer_no_tech'] = Evaluator.calculate_metrics(trans_no_tech_preds, trans_no_tech_truths)\n",
    "\n",
    "        informer_no_tech = InformerModel(input_size_no_tech, **Config.model_config['Informer'], dropout=Config.dropout, use_positional_encoding=True, use_probs_sparse_attn=True)\n",
    "        informer_no_tech_trainer = Trainer(informer_no_tech, 'Informer_no_tech')\n",
    "        informer_no_tech_preds, informer_no_tech_truths = informer_no_tech_trainer.train(train_loader_no_tech, test_loader_no_tech)\n",
    "        model_results['Informer_no_tech'] = Evaluator.calculate_metrics(informer_no_tech_preds, informer_no_tech_truths)\n",
    "\n",
    "        results[symbol] = model_results\n",
    "\n",
    "        for model_name, preds in [\n",
    "            ('Transformer', trans_preds), ('Transformer_no_pe', trans_no_pe_preds),\n",
    "            ('Informer', informer_preds), ('Informer_no_pe', informer_no_pe_preds),\n",
    "            ('Informer_no_probs', informer_no_probs_preds), ('Transformer_no_imf', trans_no_imf_preds),\n",
    "            ('Informer_no_imf', informer_no_imf_preds), ('Transformer_no_tech', trans_no_tech_preds),\n",
    "            ('Informer_no_tech', informer_no_tech_preds)\n",
    "        ]:\n",
    "            # \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(trans_truths, label='True Returns', alpha=0.7)\n",
    "            plt.plot(preds, label=f'{model_name} Predicted Returns', linestyle='--')\n",
    "            plt.title(f'{symbol} Return Prediction - {model_name}')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'{symbol}_prediction_ending1_{model_name}.png')\n",
    "            plt.close()\n",
    "\n",
    "            # ROC\n",
    "            pred_dir = (preds > 0).astype(int)\n",
    "            true_dir = (trans_truths > 0).astype(int)\n",
    "            fpr, tpr, thresholds = roc_curve(true_dir, preds)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title(f'{symbol} {model_name} ROC Curve')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.savefig(f'{symbol}_roc_curve_{model_name}.png')\n",
    "            plt.close()\n",
    "\n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    for symbol in results:\n",
    "        print(f\"\\n{symbol} Performance:\")\n",
    "        for model in results[symbol]:\n",
    "            metrics = results[symbol][model]\n",
    "            print(f\"{model}:\")\n",
    "            print(f\"  RMSE: {metrics['RMSE']:.4f} | MAE: {metrics['MAE']:.4f}\")\n",
    "            print(f\"  Accuracy: {metrics['Accuracy']:.2%} | Recall: {metrics['Recall']:.2%}\")\n",
    "            print(f\"  Sharpe: {metrics['Sharpe']:.2f} | Corr: {metrics['Correlation']:.2f}\")\n",
    "            print(f\"  Precision: {metrics['Precision']:.2f}\")\n",
    "            print(f\"  F1: {metrics['F1']:.2f}\")\n",
    "            print(f\"  AUC: {metrics['AUC']:.2f}\")\n",
    "            print(f\"  Confusion Matrix:\\n{metrics['ConfusionMatrix']}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c95e7e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files generated for each stock: AAPL, MSFT, AMZN, GOOG, META, NVDA, TSLA\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Data for each stock\n",
    "stocks_data = {\n",
    "    \"AAPL\": [\n",
    "        {\"Model\": \"Transformer\", \"RMSE\": 0.5499, \"MAE\": 0.3245, \"Accuracy (%)\": 88.07, \"Recall (%)\": 79.65, \"Sharpe\": -3.96, \"Corr\": 0.88, \"Precision\": 0.97, \"F1\": 0.87, \"AUC\": 0.96, \"Confusion Matrix\": \"[[309, 9], [70, 274]]\"},\n",
    "        {\"Model\": \"Transformer_no_pe\", \"RMSE\": 0.9128, \"MAE\": 0.6650, \"Accuracy (%)\": 69.18, \"Recall (%)\": 64.24, \"Sharpe\": -2.19, \"Corr\": 0.49, \"Precision\": 0.73, \"F1\": 0.68, \"AUC\": 0.76, \"Confusion Matrix\": \"[[237, 81], [123, 221]]\"},\n",
    "        {\"Model\": \"Informer\", \"RMSE\": 0.4646, \"MAE\": 0.3171, \"Accuracy (%)\": 86.25, \"Recall (%)\": 75.87, \"Sharpe\": -4.58, \"Corr\": 0.91, \"Precision\": 0.97, \"F1\": 0.85, \"AUC\": 0.96, \"Confusion Matrix\": \"[[310, 8], [83, 261]]\"},\n",
    "        {\"Model\": \"Informer_no_pe\", \"RMSE\": 1.0586, \"MAE\": 0.8068, \"Accuracy (%)\": 68.88, \"Recall (%)\": 57.27, \"Sharpe\": -3.19, \"Corr\": 0.43, \"Precision\": 0.77, \"F1\": 0.66, \"AUC\": 0.75, \"Confusion Matrix\": \"[[259, 59], [147, 197]]\"},\n",
    "        {\"Model\": \"Informer_no_probs\", \"RMSE\": 0.4185, \"MAE\": 0.2888, \"Accuracy (%)\": 88.37, \"Recall (%)\": 82.85, \"Sharpe\": -3.40, \"Corr\": 0.92, \"Precision\": 0.94, \"F1\": 0.88, \"AUC\": 0.96, \"Confusion Matrix\": \"[[300, 18], [59, 285]]\"},\n",
    "        {\"Model\": \"Transformer_no_imf\", \"RMSE\": 0.6264, \"MAE\": 0.4659, \"Accuracy (%)\": 80.97, \"Recall (%)\": 70.06, \"Sharpe\": -5.59, \"Corr\": 0.82, \"Precision\": 0.91, \"F1\": 0.79, \"AUC\": 0.91, \"Confusion Matrix\": \"[[295, 23], [103, 241]]\"},\n",
    "        {\"Model\": \"Informer_no_imf\", \"RMSE\": 0.6121, \"MAE\": 0.4376, \"Accuracy (%)\": 80.51, \"Recall (%)\": 80.23, \"Sharpe\": -1.62, \"Corr\": 0.81, \"Precision\": 0.82, \"F1\": 0.81, \"AUC\": 0.89, \"Confusion Matrix\": \"[[257, 61], [68, 276]]\"},\n",
    "        {\"Model\": \"Transformer_no_tech\", \"RMSE\": 0.5785, \"MAE\": 0.3540, \"Accuracy (%)\": 84.29, \"Recall (%)\": 71.51, \"Sharpe\": -5.22, \"Corr\": 0.88, \"Precision\": 0.98, \"F1\": 0.83, \"AUC\": 0.95, \"Confusion Matrix\": \"[[312, 6], [98, 246]]\"},\n",
    "        {\"Model\": \"Informer_no_tech\", \"RMSE\": 0.5515, \"MAE\": 0.3853, \"Accuracy (%)\": 81.12, \"Recall (%)\": 65.12, \"Sharpe\": -6.96, \"Corr\": 0.90, \"Precision\": 0.98, \"F1\": 0.78, \"AUC\": 0.95, \"Confusion Matrix\": \"[[313, 5], [120, 224]]\"},\n",
    "    ],\n",
    "    \"MSFT\": [\n",
    "        {\"Model\": \"Transformer\", \"RMSE\": 0.5499, \"MAE\": 0.3790, \"Accuracy (%)\": 85.50, \"Recall (%)\": 75.95, \"Sharpe\": -3.88, \"Corr\": 0.87, \"Precision\": 0.95, \"F1\": 0.84, \"AUC\": 0.95, \"Confusion Matrix\": \"[[307, 14], [82, 259]]\"},\n",
    "        {\"Model\": \"Transformer_no_pe\", \"RMSE\": 0.9405, \"MAE\": 0.7159, \"Accuracy (%)\": 68.43, \"Recall (%)\": 70.38, \"Sharpe\": -0.77, \"Corr\": 0.49, \"Precision\": 0.69, \"F1\": 0.70, \"AUC\": 0.74, \"Confusion Matrix\": \"[[213, 108], [101, 240]]\"},\n",
    "        {\"Model\": \"Informer\", \"RMSE\": 0.5900, \"MAE\": 0.3636, \"Accuracy (%)\": 87.46, \"Recall (%)\": 84.16, \"Sharpe\": -1.27, \"Corr\": 0.82, \"Precision\": 0.91, \"F1\": 0.87, \"AUC\": 0.94, \"Confusion Matrix\": \"[[292, 29], [54, 287]]\"},\n",
    "        {\"Model\": \"Informer_no_pe\", \"RMSE\": 1.0870, \"MAE\": 0.8465, \"Accuracy (%)\": 65.71, \"Recall (%)\": 55.72, \"Sharpe\": -5.37, \"Corr\": 0.40, \"Precision\": 0.71, \"F1\": 0.63, \"AUC\": 0.71, \"Confusion Matrix\": \"[[245, 76], [151, 190]]\"},\n",
    "        {\"Model\": \"Informer_no_probs\", \"RMSE\": 0.6028, \"MAE\": 0.3704, \"Accuracy (%)\": 88.07, \"Recall (%)\": 88.27, \"Sharpe\": 0.58, \"Corr\": 0.81, \"Precision\": 0.89, \"F1\": 0.88, \"AUC\": 0.95, \"Confusion Matrix\": \"[[282, 39], [40, 301]]\"},\n",
    "        {\"Model\": \"Transformer_no_imf\", \"RMSE\": 0.6637, \"MAE\": 0.5263, \"Accuracy (%)\": 77.79, \"Recall (%)\": 65.10, \"Sharpe\": -4.54, \"Corr\": 0.81, \"Precision\": 0.89, \"F1\": 0.75, \"AUC\": 0.90, \"Confusion Matrix\": \"[[293, 28], [119, 222]]\"},\n",
    "        {\"Model\": \"Informer_no_imf\", \"RMSE\": 0.6258, \"MAE\": 0.4839, \"Accuracy (%)\": 81.12, \"Recall (%)\": 91.50, \"Sharpe\": 3.56, \"Corr\": 0.82, \"Precision\": 0.76, \"F1\": 0.83, \"AUC\": 0.91, \"Confusion Matrix\": \"[[225, 96], [29, 312]]\"},\n",
    "        {\"Model\": \"Transformer_no_tech\", \"RMSE\": 0.5106, \"MAE\": 0.3266, \"Accuracy (%)\": 88.97, \"Recall (%)\": 92.96, \"Sharpe\": 0.79, \"Corr\": 0.87, \"Precision\": 0.87, \"F1\": 0.90, \"AUC\": 0.96, \"Confusion Matrix\": \"[[272, 49], [24, 317]]\"},\n",
    "        {\"Model\": \"Informer_no_tech\", \"RMSE\": 0.5502, \"MAE\": 0.3696, \"Accuracy (%)\": 87.31, \"Recall (%)\": 90.32, \"Sharpe\": 0.86, \"Corr\": 0.86, \"Precision\": 0.86, \"F1\": 0.88, \"AUC\": 0.94, \"Confusion Matrix\": \"[[270, 51], [33, 308]]\"},\n",
    "    ],\n",
    "    \"AMZN\": [\n",
    "        {\"Model\": \"Transformer\", \"RMSE\": 0.4496, \"MAE\": 0.3190, \"Accuracy (%)\": 88.37, \"Recall (%)\": 87.02, \"Sharpe\": -1.79, \"Corr\": 0.91, \"Precision\": 0.90, \"F1\": 0.88, \"AUC\": 0.96, \"Confusion Matrix\": \"[[290, 33], [44, 295]]\"},\n",
    "        {\"Model\": \"Transformer_no_pe\", \"RMSE\": 1.1142, \"MAE\": 0.8202, \"Accuracy (%)\": 68.73, \"Recall (%)\": 81.42, \"Sharpe\": 2.06, \"Corr\": 0.48, \"Precision\": 0.66, \"F1\": 0.73, \"AUC\": 0.73, \"Confusion Matrix\": \"[[179, 144], [63, 276]]\"},\n",
    "        {\"Model\": \"Informer\", \"RMSE\": 0.4895, \"MAE\": 0.3387, \"Accuracy (%)\": 87.46, \"Recall (%)\": 87.61, \"Sharpe\": -1.18, \"Corr\": 0.91, \"Precision\": 0.88, \"F1\": 0.88, \"AUC\": 0.94, \"Confusion Matrix\": \"[[282, 41], [42, 297]]\"},\n",
    "        {\"Model\": \"Informer_no_pe\", \"RMSE\": 0.9978, \"MAE\": 0.7625, \"Accuracy (%)\": 65.86, \"Recall (%)\": 67.55, \"Sharpe\": -2.68, \"Corr\": 0.49, \"Precision\": 0.66, \"F1\": 0.67, \"AUC\": 0.70, \"Confusion Matrix\": \"[[207, 116], [110, 229]]\"},\n",
    "        {\"Model\": \"Informer_no_probs\", \"RMSE\": 0.5060, \"MAE\": 0.3461, \"Accuracy (%)\": 88.67, \"Recall (%)\": 91.15, \"Sharpe\": -0.61, \"Corr\": 0.90, \"Precision\": 0.87, \"F1\": 0.89, \"AUC\": 0.95, \"Confusion Matrix\": \"[[278, 45], [30, 309]]\"},\n",
    "        {\"Model\": \"Transformer_no_imf\", \"RMSE\": 0.5854, \"MAE\": 0.4350, \"Accuracy (%)\": 80.82, \"Recall (%)\": 78.76, \"Sharpe\": -1.06, \"Corr\": 0.84, \"Precision\": 0.83, \"F1\": 0.81, \"AUC\": 0.90, \"Confusion Matrix\": \"[[268, 55], [72, 267]]\"},\n",
    "        {\"Model\": \"Informer_no_imf\", \"RMSE\": 0.5716, \"MAE\": 0.4243, \"Accuracy (%)\": 82.02, \"Recall (%)\": 87.02, \"Sharpe\": 0.96, \"Corr\": 0.86, \"Precision\": 0.80, \"F1\": 0.83, \"AUC\": 0.91, \"Confusion Matrix\": \"[[248, 75], [44, 295]]\"},\n",
    "        {\"Model\": \"Transformer_no_tech\", \"RMSE\": 0.4593, \"MAE\": 0.3238, \"Accuracy (%)\": 88.82, \"Recall (%)\": 93.22, \"Sharpe\": 0.89, \"Corr\": 0.91, \"Precision\": 0.86, \"F1\": 0.90, \"AUC\": 0.96, \"Confusion Matrix\": \"[[272, 51], [23, 316]]\"},\n",
    "        {\"Model\": \"Informer_no_tech\", \"RMSE\": 0.4939, \"MAE\": 0.3503, \"Accuracy (%)\": 88.22, \"Recall (%)\": 93.81, \"Sharpe\": 0.74, \"Corr\": 0.91, \"Precision\": 0.85, \"F1\": 0.89, \"AUC\": 0.95, \"Confusion Matrix\": \"[[266, 57], [21, 318]]\"},\n",
    "    ],\n",
    "    \"GOOG\": [\n",
    "        {\"Model\": \"Transformer\", \"RMSE\": 0.5648, \"MAE\": 0.4000, \"Accuracy (%)\": 84.44, \"Recall (%)\": 82.34, \"Sharpe\": -1.35, \"Corr\": 0.88, \"Precision\": 0.86, \"F1\": 0.84, \"AUC\": 0.93, \"Confusion Matrix\": \"[[284, 44], [59, 275]]\"},\n",
    "        {\"Model\": \"Transformer_no_pe\", \"RMSE\": 1.1464, \"MAE\": 0.8612, \"Accuracy (%)\": 62.39, \"Recall (%)\": 55.39, \"Sharpe\": -2.22, \"Corr\": 0.43, \"Precision\": 0.65, \"F1\": 0.60, \"AUC\": 0.68, \"Confusion Matrix\": \"[[228, 100], [149, 185]]\"},\n",
    "        {\"Model\": \"Informer\", \"RMSE\": 0.6075, \"MAE\": 0.4390, \"Accuracy (%)\": 87.16, \"Recall (%)\": 88.92, \"Sharpe\": 0.41, \"Corr\": 0.90, \"Precision\": 0.86, \"F1\": 0.87, \"AUC\": 0.95, \"Confusion Matrix\": \"[[280, 48], [37, 297]]\"},\n",
    "        {\"Model\": \"Informer_no_pe\", \"RMSE\": 1.0838, \"MAE\": 0.8129, \"Accuracy (%)\": 62.08, \"Recall (%)\": 48.20, \"Sharpe\": -4.68, \"Corr\": 0.47, \"Precision\": 0.67, \"F1\": 0.56, \"AUC\": 0.72, \"Confusion Matrix\": \"[[250, 78], [173, 161]]\"},\n",
    "        {\"Model\": \"Informer_no_probs\", \"RMSE\": 0.5908, \"MAE\": 0.4139, \"Accuracy (%)\": 87.01, \"Recall (%)\": 80.54, \"Sharpe\": -2.78, \"Corr\": 0.90, \"Precision\": 0.93, \"F1\": 0.86, \"AUC\": 0.96, \"Confusion Matrix\": \"[[307, 21], [65, 269]]\"},\n",
    "        {\"Model\": \"Transformer_no_imf\", \"RMSE\": 0.8140, \"MAE\": 0.5963, \"Accuracy (%)\": 76.59, \"Recall (%)\": 58.98, \"Sharpe\": -6.29, \"Corr\": 0.79, \"Precision\": 0.92, \"F1\": 0.72, \"AUC\": 0.88, \"Confusion Matrix\": \"[[310, 18], [137, 197]]\"},\n",
    "        {\"Model\": \"Informer_no_imf\", \"RMSE\": 0.6666, \"MAE\": 0.5110, \"Accuracy (%)\": 80.51, \"Recall (%)\": 80.24, \"Sharpe\": -0.04, \"Corr\": 0.84, \"Precision\": 0.81, \"F1\": 0.81, \"AUC\": 0.90, \"Confusion Matrix\": \"[[265, 63], [66, 268]]\"},\n",
    "        {\"Model\": \"Transformer_no_tech\", \"RMSE\": 0.5283, \"MAE\": 0.3672, \"Accuracy (%)\": 86.86, \"Recall (%)\": 83.53, \"Sharpe\": -1.69, \"Corr\": 0.90, \"Precision\": 0.90, \"F1\": 0.87, \"AUC\": 0.95, \"Confusion Matrix\": \"[[296, 32], [55, 279]]\"},\n",
    "        {\"Model\": \"Informer_no_tech\", \"RMSE\": 0.5992, \"MAE\": 0.4338, \"Accuracy (%)\": 85.35, \"Recall (%)\": 77.25, \"Sharpe\": -1.96, \"Corr\": 0.89, \"Precision\": 0.92, \"F1\": 0.84, \"AUC\": 0.95, \"Confusion Matrix\": \"[[307, 21], [76, 258]]\"},\n",
    "    ],\n",
    "    \"META\": [\n",
    "        {\"Model\": \"Transformer\", \"RMSE\": 0.5021, \"MAE\": 0.3245, \"Accuracy (%)\": 88.80, \"Recall (%)\": 88.77, \"Sharpe\": 0.69, \"Corr\": 0.90, \"Precision\": 0.92, \"F1\": 0.90, \"AUC\": 0.94, \"Confusion Matrix\": \"[[239, 30], [42, 332]]\"},\n",
    "        {\"Model\": \"Transformer_no_pe\", \"RMSE\": 1.0199, \"MAE\": 0.7134, \"Accuracy (%)\": 69.52, \"Recall (%)\": 63.37, \"Sharpe\": -1.08, \"Corr\": 0.48, \"Precision\": 0.80, \"F1\": 0.71, \"AUC\": 0.76, \"Confusion Matrix\": \"[[210, 59], [137, 237]]\"},\n",
    "        {\"Model\": \"Informer\", \"RMSE\": 0.5140, \"MAE\": 0.3666, \"Accuracy (%)\": 87.09, \"Recall (%)\": 82.35, \"Sharpe\": -0.84, \"Corr\": 0.92, \"Precision\": 0.95, \"F1\": 0.88, \"AUC\": 0.95, \"Confusion Matrix\": \"[[252, 17], [66, 308]]\"},\n",
    "        {\"Model\": \"Informer_no_pe\", \"RMSE\": 1.1087, \"MAE\": 0.7953, \"Accuracy (%)\": 59.10, \"Recall (%)\": 54.28, \"Sharpe\": -0.72, \"Corr\": 0.33, \"Precision\": 0.69, \"F1\": 0.61, \"AUC\": 0.65, \"Confusion Matrix\": \"[[177, 92], [171, 203]]\"},\n",
    "        {\"Model\": \"Informer_no_probs\", \"RMSE\": 0.4432, \"MAE\": 0.3190, \"Accuracy (%)\": 88.02, \"Recall (%)\": 86.63, \"Sharpe\": 0.94, \"Corr\": 0.93, \"Precision\": 0.92, \"F1\": 0.89, \"AUC\": 0.95, \"Confusion Matrix\": \"[[242, 27], [50, 324]]\"},\n",
    "        {\"Model\": \"Transformer_no_imf\", \"RMSE\": 0.6293, \"MAE\": 0.4457, \"Accuracy (%)\": 81.34, \"Recall (%)\": 75.40, \"Sharpe\": -1.41, \"Corr\": 0.86, \"Precision\": 0.91, \"F1\": 0.82, \"AUC\": 0.91, \"Confusion Matrix\": \"[[241, 28], [92, 282]]\"},\n",
    "        {\"Model\": \"Informer_no_imf\", \"RMSE\": 0.6452, \"MAE\": 0.4990, \"Accuracy (%)\": 75.74, \"Recall (%)\": 62.83, \"Sharpe\": -4.07, \"Corr\": 0.88, \"Precision\": 0.93, \"F1\": 0.75, \"AUC\": 0.91, \"Confusion Matrix\": \"[[252, 17], [139, 235]]\"},\n",
    "        {\"Model\": \"Transformer_no_tech\", \"RMSE\": 0.4439, \"MAE\": 0.3027, \"Accuracy (%)\": 89.58, \"Recall (%)\": 93.58, \"Sharpe\": 3.18, \"Corr\": 0.93, \"Precision\": 0.89, \"F1\": 0.91, \"AUC\": 0.96, \"Confusion Matrix\": \"[[226, 43], [24, 350]]\"},\n",
    "        {\"Model\": \"Informer_no_tech\", \"RMSE\": 0.4599, \"MAE\": 0.3176, \"Accuracy (%)\": 89.11, \"Recall (%)\": 90.37, \"Sharpe\": 2.38, \"Corr\": 0.94, \"Precision\": 0.91, \"F1\": 0.91, \"AUC\": 0.96, \"Confusion Matrix\": \"[[235, 34], [36, 338]]\"},\n",
    "    ],\n",
    "    \"NVDA\": [\n",
    "        {\"Model\": \"Transformer\", \"RMSE\": 0.5636, \"MAE\": 0.4279, \"Accuracy (%)\": 87.46, \"Recall (%)\": 84.66, \"Sharpe\": -0.94, \"Corr\": 0.90, \"Precision\": 0.92, \"F1\": 0.88, \"AUC\": 0.94, \"Confusion Matrix\": \"[[270, 27], [56, 309]]\"},\n",
    "        {\"Model\": \"Transformer_no_pe\", \"RMSE\": 1.1898, \"MAE\": 0.9589, \"Accuracy (%)\": 62.24, \"Recall (%)\": 68.49, \"Sharpe\": 2.22, \"Corr\": 0.38, \"Precision\": 0.65, \"F1\": 0.67, \"AUC\": 0.68, \"Confusion Matrix\": \"[[162, 135], [115, 250]]\"},\n",
    "        {\"Model\": \"Informer\", \"RMSE\": 0.5646, \"MAE\": 0.4152, \"Accuracy (%)\": 85.20, \"Recall (%)\": 81.10, \"Sharpe\": -0.45, \"Corr\": 0.89, \"Precision\": 0.91, \"F1\": 0.86, \"AUC\": 0.94, \"Confusion Matrix\": \"[[268, 29], [69, 296]]\"},\n",
    "        {\"Model\": \"Informer_no_pe\", \"RMSE\": 1.4298, \"MAE\": 1.0872, \"Accuracy (%)\": 64.95, \"Recall (%)\": 69.59, \"Sharpe\": -0.90, \"Corr\": 0.38, \"Precision\": 0.68, \"F1\": 0.69, \"AUC\": 0.69, \"Confusion Matrix\": \"[[176, 121], [111, 254]]\"},\n",
    "        {\"Model\": \"Informer_no_probs\", \"RMSE\": 0.5152, \"MAE\": 0.3758, \"Accuracy (%)\": 86.71, \"Recall (%)\": 90.14, \"Sharpe\": 2.72, \"Corr\": 0.90, \"Precision\": 0.86, \"F1\": 0.88, \"AUC\": 0.94, \"Confusion Matrix\": \"[[245, 52], [36, 329]]\"},\n",
    "        {\"Model\": \"Transformer_no_imf\", \"RMSE\": 0.6546, \"MAE\": 0.5042, \"Accuracy (%)\": 83.69, \"Recall (%)\": 83.56, \"Sharpe\": 0.63, \"Corr\": 0.83, \"Precision\": 0.86, \"F1\": 0.85, \"AUC\": 0.91, \"Confusion Matrix\": \"[[249, 48], [60, 305]]\"},\n",
    "        {\"Model\": \"Informer_no_imf\", \"RMSE\": 0.6780, \"MAE\": 0.5255, \"Accuracy (%)\": 81.42, \"Recall (%)\": 79.73, \"Sharpe\": 0.21, \"Corr\": 0.82, \"Precision\": 0.86, \"F1\": 0.83, \"AUC\": 0.90, \"Confusion Matrix\": \"[[248, 49], [74, 291]]\"},\n",
    "        {\"Model\": \"Transformer_no_tech\", \"RMSE\": 0.5189, \"MAE\": 0.3881, \"Accuracy (%)\": 84.59, \"Recall (%)\": 80.55, \"Sharpe\": -0.43, \"Corr\": 0.90, \"Precision\": 0.90, \"F1\": 0.85, \"AUC\": 0.94, \"Confusion Matrix\": \"[[266, 31], [71, 294]]\"},\n",
    "        {\"Model\": \"Informer_no_tech\", \"RMSE\": 0.5446, \"MAE\": 0.4143, \"Accuracy (%)\": 86.56, \"Recall (%)\": 89.59, \"Sharpe\": 1.51, \"Corr\": 0.91, \"Precision\": 0.87, \"F1\": 0.88, \"AUC\": 0.94, \"Confusion Matrix\": \"[[246, 51], [38, 327]]\"},\n",
    "    ],\n",
    "    \"TSLA\": [\n",
    "        {\"Model\": \"Transformer\", \"RMSE\": 0.5020, \"MAE\": 0.3946, \"Accuracy (%)\": 86.25, \"Recall (%)\": 80.00, \"Sharpe\": -1.80, \"Corr\": 0.91, \"Precision\": 0.89, \"F1\": 0.84, \"AUC\": 0.94, \"Confusion Matrix\": \"[[331, 31], [60, 240]]\"},\n",
    "        {\"Model\": \"Transformer_no_pe\", \"RMSE\": 1.0492, \"MAE\": 0.8135, \"Accuracy (%)\": 69.64, \"Recall (%)\": 63.33, \"Sharpe\": -2.89, \"Corr\": 0.57, \"Precision\": 0.68, \"F1\": 0.65, \"AUC\": 0.77, \"Confusion Matrix\": \"[[271, 91], [110, 190]]\"},\n",
    "        {\"Model\": \"Informer\", \"RMSE\": 0.4568, \"MAE\": 0.3513, \"Accuracy (%)\": 88.07, \"Recall (%)\": 85.67, \"Sharpe\": -1.39, \"Corr\": 0.93, \"Precision\": 0.88, \"F1\": 0.87, \"AUC\": 0.95, \"Confusion Matrix\": \"[[326, 36], [43, 257]]\"},\n",
    "        {\"Model\": \"Informer_no_pe\", \"RMSE\": 1.1764, \"MAE\": 0.9404, \"Accuracy (%)\": 66.31, \"Recall (%)\": 65.67, \"Sharpe\": 3.42, \"Corr\": 0.48, \"Precision\": 0.62, \"F1\": 0.64, \"AUC\": 0.74, \"Confusion Matrix\": \"[[242, 120], [103, 197]]\"},\n",
    "        {\"Model\": \"Informer_no_probs\", \"RMSE\": 0.4218, \"MAE\": 0.3211, \"Accuracy (%)\": 87.92, \"Recall (%)\": 82.67, \"Sharpe\": -3.12, \"Corr\": 0.93, \"Precision\": 0.90, \"F1\": 0.86, \"AUC\": 0.96, \"Confusion Matrix\": \"[[334, 28], [52, 248]]\"},\n",
    "        {\"Model\": \"Transformer_no_imf\", \"RMSE\": 0.5867, \"MAE\": 0.4492, \"Accuracy (%)\": 81.72, \"Recall (%)\": 88.00, \"Sharpe\": 1.20, \"Corr\": 0.86, \"Precision\": 0.76, \"F1\": 0.81, \"AUC\": 0.92, \"Confusion Matrix\": \"[[277, 85], [36, 264]]\"},\n",
    "        {\"Model\": \"Informer_no_imf\", \"RMSE\": 0.6504, \"MAE\": 0.4917, \"Accuracy (%)\": 81.27, \"Recall (%)\": 76.67, \"Sharpe\": -2.03, \"Corr\": 0.84, \"Precision\": 0.81, \"F1\": 0.79, \"AUC\": 0.90, \"Confusion Matrix\": \"[[308, 54], [70, 230]]\"},\n",
    "        {\"Model\": \"Transformer_no_tech\", \"RMSE\": 0.4039, \"MAE\": 0.3030, \"Accuracy (%)\": 88.37, \"Recall (%)\": 82.67, \"Sharpe\": -2.44, \"Corr\": 0.93, \"Precision\": 0.91, \"F1\": 0.87, \"AUC\": 0.96, \"Confusion Matrix\": \"[[337, 25], [52, 248]]\"},\n",
    "        {\"Model\": \"Informer_no_tech\", \"RMSE\": 0.4671, \"MAE\": 0.3528, \"Accuracy (%)\": 89.27, \"Recall (%)\": 89.67, \"Sharpe\": -0.83, \"Corr\": 0.94, \"Precision\": 0.87, \"F1\": 0.88, \"AUC\": 0.96, \"Confusion Matrix\": \"[[322, 40], [31, 269]]\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# CSV headers\n",
    "headers = [\"Model\", \"RMSE\", \"MAE\", \"Accuracy (%)\", \"Recall (%)\", \"Sharpe\", \"Corr\", \"Precision\", \"F1\", \"AUC\", \"Confusion Matrix\"]\n",
    "\n",
    "# Generate a CSV file for each stock\n",
    "for stock, data in stocks_data.items():\n",
    "    filename = f\"{stock}_performance.csv\"\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(\"CSV files generated for each stock: AAPL, MSFT, AMZN, GOOG, META, NVDA, TSLA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f3f300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
